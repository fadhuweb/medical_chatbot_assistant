{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical LLM Fine-Tuning\n",
    "\n",
    "**Domain:** Healthcare/Medical Q&A  \n",
    "**Model:** TinyLlama-1.1B-Chat-v1.0  \n",
    "**Method:** LoRA (Low-Rank Adaptation)  \n",
    "**Dataset:** Medical Meadow Medical Flashcards  \n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements a **domain-specific medical assistant** by fine-tuning a Large Language Model (LLM) on medical question-answer pairs. The goal is to create a model that can:\n",
    "\n",
    "1. Answer medical questions accurately\n",
    "2. Provide domain-specific responses\n",
    "3. Demonstrate measurable improvement over the base model\n",
    "\n",
    "### Why This Matters:\n",
    "- Medical information requires accuracy and domain expertise\n",
    "- Pre-trained models lack specialized medical knowledge\n",
    "- Fine-tuning adapts the model to medical terminology and reasoning\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### 1. **Dataset:** \n",
    "- Medical Meadow Medical Flashcards (Hugging Face)\n",
    "- Covers diverse medical topics and terminology\n",
    "\n",
    "### 2. **Fine-tuning Approach:**\n",
    "- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning\n",
    "- Achieves similar results to full fine-tuning\n",
    "\n",
    "### 3. **Evaluation:**\n",
    "- **Quantitative**: Loss, Perplexity, BLEU, ROUGE scores\n",
    "- **Qualitative**: Compare responses before/after fine-tuning\n",
    "- **Baseline**: Evaluate pre-trained model first for comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "The required libraries for LLM fine-tuning are installed:\n",
    "\n",
    "- **transformers**: Hugging Face library for LLMs\n",
    "- **datasets**: Loading and processing datasets\n",
    "- **peft**: Parameter-Efficient Fine-Tuning (LoRA)\n",
    "- **accelerate**: Distributed training and mixed precision\n",
    "- **evaluate**: Metrics (BLEU, ROUGE, etc.)\n",
    "- **sentencepiece**: Tokenization support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:02.488416Z",
     "iopub.status.busy": "2026-02-18T21:35:02.488128Z",
     "iopub.status.idle": "2026-02-18T21:35:06.185056Z",
     "shell.execute_reply": "2026-02-18T21:35:06.184034Z",
     "shell.execute_reply.started": "2026-02-18T21:35:02.488389Z"
    },
    "id": "E0BkcVYWRRo_",
    "outputId": "548d96f3-4d8c-4c63-c712-fdf2ae712d7f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft accelerate bitsandbytes evaluate sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:57:03.454869Z",
     "iopub.status.busy": "2026-02-18T22:57:03.454588Z",
     "iopub.status.idle": "2026-02-18T22:57:03.460845Z",
     "shell.execute_reply": "2026-02-18T22:57:03.460248Z",
     "shell.execute_reply.started": "2026-02-18T22:57:03.454844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,TrainingArguments,Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import shutil\n",
    "import evaluate\n",
    "import zipfile\n",
    "import glob\n",
    "\n",
    "# Seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "### Dataset: Medical Meadow Medical Flashcards\n",
    "\n",
    "This dataset contains medical question-answer pairs in the format:\n",
    "- **Instruction**: Task description (e.g., \"Answer this question truthfully\")\n",
    "- **Input**: Medical question\n",
    "- **Output**: Correct medical answer\n",
    "\n",
    "Explore the dataset structure and sample entries to understand the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:16.692432Z",
     "iopub.status.busy": "2026-02-18T21:35:16.691817Z",
     "iopub.status.idle": "2026-02-18T21:35:17.536219Z",
     "shell.execute_reply": "2026-02-18T21:35:17.535372Z",
     "shell.execute_reply.started": "2026-02-18T21:35:16.692405Z"
    },
    "id": "iaovQZXnRRpA",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 33,955\n",
      "\n",
      "Example 1:\n",
      "Q: What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ leve...\n",
      "A: Very low Mg2+ levels correspond to low PTH levels which in turn results in low C...\n",
      "\n",
      "Example 2:\n",
      "Q: What leads to genitourinary syndrome of menopause (atrophic vaginitis)?...\n",
      "A: Low estradiol production leads to genitourinary syndrome of menopause (atrophic ...\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n",
    "print(f\"Total: {len(dataset['train']):,}\")\n",
    "\n",
    "# Show samples\n",
    "for i in range(2):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Q: {dataset['train'][i]['input'][:80]}...\")\n",
    "    print(f\"A: {dataset['train'][i]['output'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Why Preprocessing Matters:\n",
    "\n",
    "Raw data often contains:\n",
    "- Missing values\n",
    "- Duplicate entries\n",
    "- Inconsistent formatting\n",
    "\n",
    "### Preprocessing Pipeline:\n",
    "\n",
    "1. **Data Cleaning**: Remove null/empty entries\n",
    "2. **Deduplication**: Remove duplicate Q&A pairs\n",
    "3. **Size Limiting**: Use 1,200 samples (GPU memory constraint)\n",
    "4. **Train/Val/Test Split**: 80/10/10 split\n",
    "\n",
    "### Key Decisions:\n",
    "- **2,0000 samples**: Balances training quality with GPU memory limits\n",
    "- **80/10/10 split**: Standard ML practice for train/validation/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:17.537850Z",
     "iopub.status.busy": "2026-02-18T21:35:17.537209Z",
     "iopub.status.idle": "2026-02-18T21:35:17.547156Z",
     "shell.execute_reply": "2026-02-18T21:35:17.546462Z",
     "shell.execute_reply.started": "2026-02-18T21:35:17.537816Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: 33,547\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning\n",
    "def clean_dataset(example):\n",
    "    return (\n",
    "        example['instruction'] and example['input'] and example['output'] and\n",
    "        len(str(example['instruction']).strip()) > 0 and\n",
    "        len(str(example['input']).strip()) > 0 and\n",
    "        len(str(example['output']).strip()) > 0\n",
    "    )\n",
    "\n",
    "cleaned = dataset['train'].filter(clean_dataset)\n",
    "print(f\"After cleaning: {len(cleaned):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:17.549383Z",
     "iopub.status.busy": "2026-02-18T21:35:17.549123Z",
     "iopub.status.idle": "2026-02-18T21:35:19.636647Z",
     "shell.execute_reply": "2026-02-18T21:35:19.636042Z",
     "shell.execute_reply.started": "2026-02-18T21:35:17.549360Z"
    },
    "id": "OdOWEyaLRRpB",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dedup: 33,521\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "df = pd.DataFrame(cleaned)\n",
    "df = df.drop_duplicates(subset=['input', 'output'])\n",
    "cleaned = Dataset.from_pandas(df)\n",
    "print(f\"After dedup: {len(cleaned):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:19.637709Z",
     "iopub.status.busy": "2026-02-18T21:35:19.637425Z",
     "iopub.status.idle": "2026-02-18T21:35:19.644857Z",
     "shell.execute_reply": "2026-02-18T21:35:19.644267Z",
     "shell.execute_reply.started": "2026-02-18T21:35:19.637676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using 2,000 samples\n"
     ]
    }
   ],
   "source": [
    "MAX_SAMPLES = 2000\n",
    "if len(cleaned) > MAX_SAMPLES:\n",
    "    cleaned = cleaned.select(range(MAX_SAMPLES))\n",
    "\n",
    "print(f\"‚úÖ Using {len(cleaned):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:19.646070Z",
     "iopub.status.busy": "2026-02-18T21:35:19.645752Z",
     "iopub.status.idle": "2026-02-18T21:35:19.664015Z",
     "shell.execute_reply": "2026-02-18T21:35:19.663444Z",
     "shell.execute_reply.started": "2026-02-18T21:35:19.646038Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,600\n",
      "Val: 200\n",
      "Test: 200\n"
     ]
    }
   ],
   "source": [
    "# Split: 80% train, 10% val, 10% test\n",
    "train_test = cleaned.train_test_split(test_size=0.2, seed=42)\n",
    "val_test = train_test['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset_split = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'validation': val_test['train'],\n",
    "    'test': val_test['test']\n",
    "})\n",
    "\n",
    "print(f\"Train: {len(dataset_split['train']):,}\")\n",
    "print(f\"Val: {len(dataset_split['validation']):,}\")\n",
    "print(f\"Test: {len(dataset_split['test']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "### Model: TinyLlama-1.1B-Chat-v1.0\n",
    "\n",
    "**Why TinyLlama?**\n",
    "- Compact (1.1B parameters vs 7B+ for larger models)\n",
    "- Fits in free GPU memory\n",
    "- Fast training\n",
    "- Good balance of quality and efficiency\n",
    "\n",
    "**Loading Configuration:**\n",
    "- `torch_dtype=torch.float16`: Half precision (saves memory)\n",
    "- `device_map=\"auto\"`: Automatically distribute across available devices\n",
    "- `low_cpu_mem_usage=True`: Minimize CPU memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:19.665053Z",
     "iopub.status.busy": "2026-02-18T21:35:19.664813Z",
     "iopub.status.idle": "2026-02-18T21:35:19.668323Z",
     "shell.execute_reply": "2026-02-18T21:35:19.667590Z",
     "shell.execute_reply.started": "2026-02-18T21:35:19.665030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:19.669502Z",
     "iopub.status.busy": "2026-02-18T21:35:19.669217Z",
     "iopub.status.idle": "2026-02-18T21:35:19.885250Z",
     "shell.execute_reply": "2026-02-18T21:35:19.884582Z",
     "shell.execute_reply.started": "2026-02-18T21:35:19.669479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(f\"‚úÖ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:19.886288Z",
     "iopub.status.busy": "2026-02-18T21:35:19.886032Z",
     "iopub.status.idle": "2026-02-18T21:35:22.368515Z",
     "shell.execute_reply": "2026-02-18T21:35:22.367705Z",
     "shell.execute_reply.started": "2026-02-18T21:35:19.886264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,  \n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:22.370031Z",
     "iopub.status.busy": "2026-02-18T21:35:22.369590Z",
     "iopub.status.idle": "2026-02-18T21:35:22.375512Z",
     "shell.execute_reply": "2026-02-18T21:35:22.374698Z",
     "shell.execute_reply.started": "2026-02-18T21:35:22.369993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: 1,100,048,384 params\n",
      "GPU Memory: 0.94 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚úÖ Model loaded: {model.num_parameters():,} params\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "\n",
    "### What is LoRA?\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning technique:\n",
    "\n",
    "- Traditional fine-tuning: Updates ALL 1.1B parameters\n",
    "-  oRA: Updates only ~1% of parameters (via low-rank matrices)\n",
    "\n",
    "### LoRA Settings:\n",
    "\n",
    "```\n",
    "r=16              # Rank of update matrices (higher = more capacity)\n",
    "lora_alpha=32     # Scaling factor\n",
    "target_modules    # Which layers to adapt (attention layers)\n",
    "lora_dropout=0.05 # Regularization\n",
    "```\n",
    "\n",
    "### Memory Savings:\n",
    "- Full fine-tuning: ~20GB GPU memory\n",
    "- LoRA: ~6-8GB GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:22.376745Z",
     "iopub.status.busy": "2026-02-18T21:35:22.376478Z",
     "iopub.status.idle": "2026-02-18T21:35:22.390182Z",
     "shell.execute_reply": "2026-02-18T21:35:22.389305Z",
     "shell.execute_reply.started": "2026-02-18T21:35:22.376710Z"
    },
    "id": "1PwhvkFOh9ic",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:22.391366Z",
     "iopub.status.busy": "2026-02-18T21:35:22.391098Z",
     "iopub.status.idle": "2026-02-18T21:35:22.400625Z",
     "shell.execute_reply": "2026-02-18T21:35:22.399845Z",
     "shell.execute_reply.started": "2026-02-18T21:35:22.391342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:22.404222Z",
     "iopub.status.busy": "2026-02-18T21:35:22.403972Z",
     "iopub.status.idle": "2026-02-18T21:35:25.919806Z",
     "shell.execute_reply": "2026-02-18T21:35:25.918954Z",
     "shell.execute_reply.started": "2026-02-18T21:35:22.404200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:25.921799Z",
     "iopub.status.busy": "2026-02-18T21:35:25.920898Z",
     "iopub.status.idle": "2026-02-18T21:35:25.930957Z",
     "shell.execute_reply": "2026-02-18T21:35:25.930180Z",
     "shell.execute_reply.started": "2026-02-18T21:35:25.921750Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA applied\n",
      "Trainable: 4,505,600 (0.41%)\n"
     ]
    }
   ],
   "source": [
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"‚úÖ LoRA applied\")\n",
    "print(f\"Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### What is Tokenization?\n",
    "\n",
    "Tokenization converts text into numbers that the model can process. For example:\n",
    "\n",
    "```\n",
    "\"What is diabetes?\" ‚Üí [2385, 310, 652, 9790, 29973]\n",
    "```\n",
    "\n",
    "### Tokenization Strategy:\n",
    "\n",
    "1. **Format**: Combine instruction + question + answer into training format\n",
    "2. **Max Length**: 256 tokens (balance quality vs memory)\n",
    "3. **Padding**: Pad all sequences to same length for batching\n",
    "4. **Labels**: Copy input_ids for causal language modeling\n",
    "\n",
    "### Why 256 tokens?\n",
    "- Medical Q&A pairs are typically short\n",
    "- Longer sequences = more GPU memory\n",
    "- 256 tokens = good balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:25.932089Z",
     "iopub.status.busy": "2026-02-18T21:35:25.931854Z",
     "iopub.status.idle": "2026-02-18T21:35:25.947021Z",
     "shell.execute_reply": "2026-02-18T21:35:25.946223Z",
     "shell.execute_reply.started": "2026-02-18T21:35:25.932066Z"
    },
    "id": "phwIDkt9h4Lg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_instruction(example):\n",
    "    instruction = example['instruction']\n",
    "    question = example['input']\n",
    "    answer = example['output']\n",
    "    prompt = f\"<|user|>\\n{instruction}\\n{question}\\n<|assistant|>\\n{answer}{tokenizer.eos_token}\"\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:25.948415Z",
     "iopub.status.busy": "2026-02-18T21:35:25.948041Z",
     "iopub.status.idle": "2026-02-18T21:35:26.288570Z",
     "shell.execute_reply": "2026-02-18T21:35:26.287606Z",
     "shell.execute_reply.started": "2026-02-18T21:35:25.948391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea51ae4148e430b9966c5138e2fe6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4a6ff2e4664fba8841a11641e424ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0495b03f874d0599190d829d7d5ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted = dataset_split.map(\n",
    "    format_instruction,\n",
    "    remove_columns=dataset_split['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:26.290112Z",
     "iopub.status.busy": "2026-02-18T21:35:26.289664Z",
     "iopub.status.idle": "2026-02-18T21:35:26.294276Z",
     "shell.execute_reply": "2026-02-18T21:35:26.293511Z",
     "shell.execute_reply.started": "2026-02-18T21:35:26.290071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"][:]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:26.295561Z",
     "iopub.status.busy": "2026-02-18T21:35:26.295271Z",
     "iopub.status.idle": "2026-02-18T21:35:26.902103Z",
     "shell.execute_reply": "2026-02-18T21:35:26.901212Z",
     "shell.execute_reply.started": "2026-02-18T21:35:26.295528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1e6e0a785147c2b715d92feff0ed02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7066167afc0c4930a4bb5970c6e98580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2438cfa0925940708ef02733fc939ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized = formatted.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:26.903364Z",
     "iopub.status.busy": "2026-02-18T21:35:26.903096Z",
     "iopub.status.idle": "2026-02-18T21:35:26.907957Z",
     "shell.execute_reply": "2026-02-18T21:35:26.907162Z",
     "shell.execute_reply.started": "2026-02-18T21:35:26.903339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train: 1,600, Val: 200\n",
      "Max length: 256 tokens\n"
     ]
    }
   ],
   "source": [
    "train_data = tokenized['train']\n",
    "val_data = tokenized['validation']\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_data):,}, Val: {len(val_data):,}\")\n",
    "print(f\"Max length: 256 tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Setup\n",
    "\n",
    "### Metrics that are used:\n",
    "\n",
    "1. **Loss**: How well the model predicts next tokens (lower is better)\n",
    "2. **Perplexity**: Exp(loss), measures uncertainty (lower is better)\n",
    "3. **BLEU**: Measures n-gram overlap with reference (0-100, higher is better)\n",
    "4. **ROUGE**: Measures recall of n-grams (0-1, higher is better)\n",
    "\n",
    "### Why Multiple Metrics?\n",
    "\n",
    "- **Loss/Perplexity**: Overall model quality\n",
    "- **BLEU**: Precision of generated text\n",
    "- **ROUGE**: Recall/coverage of key information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:26.909041Z",
     "iopub.status.busy": "2026-02-18T21:35:26.908765Z",
     "iopub.status.idle": "2026-02-18T21:35:28.905170Z",
     "shell.execute_reply": "2026-02-18T21:35:28.904421Z",
     "shell.execute_reply.started": "2026-02-18T21:35:26.909018Z"
    },
    "id": "T3JKL5JQRRpE",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metrics configured (loss, perplexity, BLEU & ROUGE)\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    return np.exp(loss)\n",
    "\n",
    "print(\"‚úÖ Metrics configured (loss, perplexity, BLEU & ROUGE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OBGeWo5jGKN"
   },
   "source": [
    "## Baseline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:28.906862Z",
     "iopub.status.busy": "2026-02-18T21:35:28.906110Z",
     "iopub.status.idle": "2026-02-18T21:35:29.314324Z",
     "shell.execute_reply": "2026-02-18T21:35:29.313551Z",
     "shell.execute_reply.started": "2026-02-18T21:35:28.906834Z"
    },
    "id": "fgmRh1BBjN-L",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clean memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:29.316270Z",
     "iopub.status.busy": "2026-02-18T21:35:29.315615Z",
     "iopub.status.idle": "2026-02-18T21:35:29.361718Z",
     "shell.execute_reply": "2026-02-18T21:35:29.361040Z",
     "shell.execute_reply.started": "2026-02-18T21:35:29.316231Z"
    },
    "id": "5f8hlhRojQnL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "baseline_args = TrainingArguments(\n",
    "    output_dir=\"./baseline\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    prediction_loss_only=True,\n",
    "    dataloader_num_workers=0,  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:29.362883Z",
     "iopub.status.busy": "2026-02-18T21:35:29.362590Z",
     "iopub.status.idle": "2026-02-18T21:35:29.383513Z",
     "shell.execute_reply": "2026-02-18T21:35:29.383010Z",
     "shell.execute_reply.started": "2026-02-18T21:35:29.362860Z"
    },
    "id": "jDyoKclOjVEI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "baseline_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=baseline_args,\n",
    "    eval_dataset=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:29.384671Z",
     "iopub.status.busy": "2026-02-18T21:35:29.384375Z",
     "iopub.status.idle": "2026-02-18T21:35:42.867199Z",
     "shell.execute_reply": "2026-02-18T21:35:42.866493Z",
     "shell.execute_reply.started": "2026-02-18T21:35:29.384644Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_metrics = baseline_trainer.evaluate()\n",
    "baseline_loss = baseline_metrics['eval_loss']\n",
    "baseline_perplexity = compute_perplexity(baseline_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:35:42.869100Z",
     "iopub.status.busy": "2026-02-18T21:35:42.868257Z",
     "iopub.status.idle": "2026-02-18T21:37:41.070675Z",
     "shell.execute_reply": "2026-02-18T21:37:41.069960Z",
     "shell.execute_reply.started": "2026-02-18T21:35:42.869068Z"
    },
    "id": "HG08yUgtjckb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "baseline_predictions = []\n",
    "baseline_references = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(min(50, len(val_data))):\n",
    "        sample = val_data[i]\n",
    "        input_ids = torch.tensor([sample[\"input_ids\"][:128]]).to(model.device)\n",
    "        attention_mask = torch.tensor([[1] * len(sample[\"input_ids\"][:128])]).to(model.device)\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        ref_text = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "        baseline_predictions.append(pred_text)\n",
    "        baseline_references.append([ref_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:37:41.072140Z",
     "iopub.status.busy": "2026-02-18T21:37:41.071723Z",
     "iopub.status.idle": "2026-02-18T21:37:41.413423Z",
     "shell.execute_reply": "2026-02-18T21:37:41.412810Z",
     "shell.execute_reply.started": "2026-02-18T21:37:41.072105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "baseline_bleu_result = bleu_metric.compute(predictions=baseline_predictions, references=baseline_references)\n",
    "baseline_bleu = baseline_bleu_result[\"bleu\"] * 100  # Convert to percentage\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "baseline_rouge_result = rouge_metric.compute(predictions=baseline_predictions, references=baseline_references)\n",
    "baseline_rouge_l = baseline_rouge_result[\"rougeL\"] * 100  # ROUGE-L F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:37:41.414527Z",
     "iopub.status.busy": "2026-02-18T21:37:41.414267Z",
     "iopub.status.idle": "2026-02-18T21:37:41.419681Z",
     "shell.execute_reply": "2026-02-18T21:37:41.419145Z",
     "shell.execute_reply.started": "2026-02-18T21:37:41.414501Z"
    },
    "id": "fZwcNXdqjfqL",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE METRICS\n",
      "================================================================================\n",
      "   Loss: 11.8996\n",
      "   Perplexity: 147204.22\n",
      "   BLEU: 67.67\n",
      "   ROGUE: 76.57\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"BASELINE METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Loss: {baseline_loss:.4f}\")\n",
    "print(f\"   Perplexity: {baseline_perplexity:.2f}\")\n",
    "print(f\"   BLEU: {baseline_bleu:.2f}\")\n",
    "print(f\"   ROGUE: {baseline_rouge_l:.2f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:37:41.421145Z",
     "iopub.status.busy": "2026-02-18T21:37:41.420753Z",
     "iopub.status.idle": "2026-02-18T21:37:41.864245Z",
     "shell.execute_reply": "2026-02-18T21:37:41.863563Z",
     "shell.execute_reply.started": "2026-02-18T21:37:41.421119Z"
    },
    "id": "Qx6EUuS1RRpE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del baseline_trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Experiments\n",
    "\n",
    "\n",
    "\n",
    "### Experimental Design:\n",
    "\n",
    "3 different configurations are tested to find the optimal hyperparameters:\n",
    "\n",
    "| Experiment | Learning Rate | Batch Size | Grad Accum | Effective Batch | Epochs | Warmup |\n",
    "|------------|--------------|------------|------------|-----------------|--------|--------|\n",
    "| **Exp1 (High LR)** | 1e-4 | 2 | 4 | 8 | 5 | 50 |\n",
    "| **Exp2 (Medium LR)** | 5e-5 | 1 | 3 | 3 | 5 | 50 |\n",
    "| **Exp3 (Low LR)** | 2e-5 | 2 | 2 | 4 | 5 | 50 |\n",
    "\n",
    "### Hyperparameter Explanations:\n",
    "\n",
    "**Learning Rate (lr):**\n",
    "- Controls how much the model updates weights each step\n",
    "- **High (1e-4)**: Faster learning but risk of instability\n",
    "- **Medium (5e-5)**: Balanced approach (often optimal)\n",
    "- **Low (2e-5)**: Slower but more stable convergence\n",
    "\n",
    "**Batch Size:**\n",
    "- Number of samples processed together\n",
    "- Smaller batches = noisier but more frequent updates\n",
    "\n",
    "**Gradient Accumulation (accum):**\n",
    "- Accumulates gradients over multiple batches before updating\n",
    "- **Effective Batch = Batch Size √ó Accumulation**\n",
    "- Simulates larger batches without memory overflow\n",
    "\n",
    "**Epochs:**\n",
    "- Number of times model sees entire dataset\n",
    "- **5 epochs**: Sufficient for convergence with our dataset size\n",
    "- More epochs = better learning but risk of overfitting\n",
    "\n",
    "**Warmup Steps:**\n",
    "- Gradually increases learning rate at start\n",
    "- **50 steps**: Prevents instability in early training\n",
    "- Helps model adjust smoothly to the data\n",
    "\n",
    "### What is tracked for Each Experiment:\n",
    "\n",
    "1. **Training Loss**: How well the model learns the training data\n",
    "2. **Validation Loss**: Performance on unseen data (generalization)\n",
    "3. **BLEU Score**: Quality of generated medical responses (0-100%)\n",
    "4. **ROUGE-L Score**: Coverage of key information (0-100%)\n",
    "5. **Perplexity**: Model confidence (lower = better)\n",
    "6. **Improvement %**: Gain over baseline model\n",
    "7. **GPU Memory**: Peak memory usage in GB\n",
    "8. **Training Time**: Minutes to complete\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "By comparing results across experiments, we can:\n",
    "- Identify the best learning rate for our task\n",
    "- Understand hyperparameter impact on performance\n",
    "- Demonstrate systematic optimization approach\n",
    "- Select the optimal configuration for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:37:41.865568Z",
     "iopub.status.busy": "2026-02-18T21:37:41.865174Z",
     "iopub.status.idle": "2026-02-18T21:37:42.085003Z",
     "shell.execute_reply": "2026-02-18T21:37:42.084332Z",
     "shell.execute_reply.started": "2026-02-18T21:37:41.865540Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING BASELINE MODEL\n",
      "================================================================================\n",
      "\n",
      "Saving baseline model to: ./baseline_model\n",
      "‚úÖ Baseline model saved!\n",
      "\n",
      "üìÅ Baseline model files:\n",
      "   - README.md\n",
      "   - adapter_config.json\n",
      "   - adapter_model.safetensors\n",
      "   - chat_template.jinja\n",
      "   - special_tokens_map.json\n",
      "   - tokenizer.json\n",
      "   - tokenizer.model\n",
      "   - tokenizer_config.json\n",
      "\n",
      "This baseline model can be used for:\n",
      "  - Comparison with fine-tuned model\n",
      "  - Demonstrating improvement from fine-tuning\n",
      "  - A/B testing in deployment\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save baseline (non-fine-tuned) model for comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING BASELINE MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_save_path = \"./baseline_model\"\n",
    "\n",
    "# Save the base model (no LoRA, no fine-tuning)\n",
    "print(f\"\\nSaving baseline model to: {baseline_save_path}\")\n",
    "model.save_pretrained(baseline_save_path)\n",
    "tokenizer.save_pretrained(baseline_save_path)\n",
    "\n",
    "print(\"‚úÖ Baseline model saved!\")\n",
    "print(f\"\\nüìÅ Baseline model files:\")\n",
    "import os\n",
    "for file in sorted(os.listdir(baseline_save_path)):\n",
    "    print(f\"   - {file}\")\n",
    "\n",
    "print(\"\\nThis baseline model can be used for:\")\n",
    "print(\"  - Comparison with fine-tuned model\")\n",
    "print(\"  - Demonstrating improvement from fine-tuning\")\n",
    "print(\"  - A/B testing in deployment\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:37:42.086010Z",
     "iopub.status.busy": "2026-02-18T21:37:42.085779Z",
     "iopub.status.idle": "2026-02-18T21:37:42.090319Z",
     "shell.execute_reply": "2026-02-18T21:37:42.089491Z",
     "shell.execute_reply.started": "2026-02-18T21:37:42.085989Z"
    },
    "id": "A8kmHmyqRRpE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    {\"name\": \"Exp1_HighLR\", \"lr\": 1e-4, \"batch\": 2, \"accum\": 4, \"epochs\": 5, \"warmup\": 50},\n",
    "    {\"name\": \"Exp2_MediumLR\", \"lr\": 5e-5, \"batch\": 1, \"accum\": 3, \"epochs\": 5, \"warmup\": 50},\n",
    "    {\"name\": \"Exp3_LowLR\", \"lr\": 2e-5, \"batch\": 2, \"accum\": 2, \"epochs\": 5, \"warmup\": 50},\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:37:42.091401Z",
     "iopub.status.busy": "2026-02-18T21:37:42.091192Z",
     "iopub.status.idle": "2026-02-18T21:37:42.101422Z",
     "shell.execute_reply": "2026-02-18T21:37:42.100676Z",
     "shell.execute_reply.started": "2026-02-18T21:37:42.091381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "best_model_path = None\n",
    "best_val_loss = float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T21:37:42.102731Z",
     "iopub.status.busy": "2026-02-18T21:37:42.102362Z",
     "iopub.status.idle": "2026-02-18T22:56:34.841608Z",
     "shell.execute_reply": "2026-02-18T22:56:34.840923Z",
     "shell.execute_reply.started": "2026-02-18T21:37:42.102707Z"
    },
    "id": "Ue0Y2yUERRpF",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ EXPERIMENT 1/3: Exp1_HighLR\n",
      "================================================================================\n",
      "LR: 0.0001, Batch: 2, Accum: 4, Epochs: 5\n",
      "Effective batch size: 8\n",
      "--------------------------------------------------------------------------------\n",
      "Trainable parameters: 4,505,600\n",
      "\n",
      "üöÄ Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 13:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.366200</td>\n",
       "      <td>0.386527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>0.366749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.311200</td>\n",
       "      <td>0.355344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.352060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>0.353376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BLEU score...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Exp1_HighLR COMPLETED!\n",
      "================================================================================\n",
      "Train Loss:   0.5437\n",
      "Val Loss:     0.3521\n",
      "Perplexity:   1.42\n",
      "BLEU Score:   89.03%\n",
      "ROUGE-L Score: 95.11%\n",
      "\n",
      "Improvement over Baseline:\n",
      "  Loss:       +97.04%\n",
      "  Perplexity: +100.00%\n",
      "  BLEU:       +31.56%\n",
      "  ROUGE-L:    +24.21%\n",
      "\n",
      "Resources:\n",
      "  GPU Memory: 1.52 GB (peak)\n",
      "  Time:       13.56 minutes\n",
      "================================================================================\n",
      "\n",
      "üíæ Saving best model to ./best_Exp1_HighLR...\n",
      "üß™ EXPERIMENT 2/3: Exp2_MediumLR\n",
      "================================================================================\n",
      "LR: 5e-05, Batch: 1, Accum: 3, Epochs: 5\n",
      "Effective batch size: 3\n",
      "--------------------------------------------------------------------------------\n",
      "Trainable parameters: 4,505,600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1335' max='1335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1335/1335 47:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.371100</td>\n",
       "      <td>0.392259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.349500</td>\n",
       "      <td>0.377172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>0.368737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.365832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.365865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BLEU score...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Exp2_MediumLR COMPLETED!\n",
      "================================================================================\n",
      "Train Loss:   0.5717\n",
      "Val Loss:     0.3658\n",
      "Perplexity:   1.44\n",
      "BLEU Score:   88.69%\n",
      "ROUGE-L Score: 94.81%\n",
      "\n",
      "Improvement over Baseline:\n",
      "  Loss:       +96.93%\n",
      "  Perplexity: +100.00%\n",
      "  BLEU:       +31.06%\n",
      "  ROUGE-L:    +23.83%\n",
      "\n",
      "Resources:\n",
      "  GPU Memory: 6.28 GB (peak)\n",
      "  Time:       47.70 minutes\n",
      "================================================================================\n",
      "üß™ EXPERIMENT 3/3: Exp3_LowLR\n",
      "================================================================================\n",
      "LR: 2e-05, Batch: 2, Accum: 2, Epochs: 5\n",
      "Effective batch size: 4\n",
      "--------------------------------------------------------------------------------\n",
      "Trainable parameters: 4,505,600\n",
      "\n",
      "üöÄ Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 14:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.399332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.387899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.352800</td>\n",
       "      <td>0.381757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.344700</td>\n",
       "      <td>0.379419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.379074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BLEU score...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Exp3_LowLR COMPLETED!\n",
      "================================================================================\n",
      "Train Loss:   0.5902\n",
      "Val Loss:     0.3791\n",
      "Perplexity:   1.46\n",
      "BLEU Score:   74.85%\n",
      "ROUGE-L Score: 80.96%\n",
      "\n",
      "Improvement over Baseline:\n",
      "  Loss:       +96.81%\n",
      "  Perplexity: +100.00%\n",
      "  BLEU:       +10.61%\n",
      "  ROUGE-L:    +5.74%\n",
      "\n",
      "Resources:\n",
      "  GPU Memory: 2.83 GB (peak)\n",
      "  Time:       14.43 minutes\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "for idx, exp in enumerate(experiments, 1):\n",
    "\n",
    "    print(f\"üß™ EXPERIMENT {idx}/{len(experiments)}: {exp['name']}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"LR: {exp['lr']}, Batch: {exp['batch']}, Accum: {exp['accum']}, Epochs: {exp['epochs']}\")\n",
    "    print(f\"Effective batch size: {exp['batch'] * exp['accum']}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    # Reload model fresh\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    \n",
    "    if hasattr(model, \"gradient_checkpointing_disable\"):\n",
    "        model.gradient_checkpointing_disable()\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Ensure LoRA parameters require gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora\" in name.lower():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Verify trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    if trainable_params == 0:\n",
    "        print(\"‚ö†Ô∏è WARNING: No trainable parameters! Skipping this experiment.\")\n",
    "        continue\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{exp['name']}\",\n",
    "        per_device_train_batch_size=exp['batch'],\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=exp['accum'],\n",
    "        num_train_epochs=exp['epochs'],\n",
    "        learning_rate=exp['lr'],\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=exp['warmup'],\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        fp16=True,  # FP16 for GPU\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=1,\n",
    "        prediction_loss_only=True,  \n",
    "        dataloader_num_workers=0,\n",
    "        # gradient_checkpointing=True, \n",
    "    )\n",
    "    # Clean memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüöÄ Training...\")\n",
    "    start = time.time()\n",
    "\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        train_time = (time.time() - start) / 60\n",
    "\n",
    "        print(f\"\\nüìä Evaluating...\")\n",
    "        eval_metrics = trainer.evaluate()\n",
    "\n",
    "        # Extract metrics\n",
    "        train_loss = train_result.training_loss\n",
    "        val_loss = eval_metrics['eval_loss']\n",
    "        val_ppl = compute_perplexity(val_loss)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        print(\"Calculating BLEU score...\")\n",
    "        predictions = []\n",
    "        references = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(min(50, len(val_data))):\n",
    "                sample = val_data[i]\n",
    "                input_ids = torch.tensor([sample[\"input_ids\"][:128]]).to(model.device)\n",
    "                outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                ref_text = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "                predictions.append(pred_text)\n",
    "                references.append([ref_text])\n",
    "\n",
    "        bleu_result = bleu_metric.compute(predictions=predictions, references=references)\n",
    "        val_bleu = bleu_result[\"bleu\"] * 100\n",
    "\n",
    "        # Calculate ROUGE score\n",
    "        rouge_result = rouge_metric.compute(predictions=predictions, references=references)\n",
    "        val_rouge_l = rouge_result[\"rougeL\"] * 100\n",
    "        gpu_mem_gb = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "\n",
    "        # Calculate improvements\n",
    "        loss_imp = ((baseline_loss - val_loss) / baseline_loss) * 100\n",
    "        ppl_imp = ((baseline_perplexity - val_ppl) / baseline_perplexity) * 100\n",
    "        bleu_imp = ((val_bleu - baseline_bleu) / max(baseline_bleu, 0.01)) * 100\n",
    "        rouge_imp = ((val_rouge_l - baseline_rouge_l) / max(baseline_rouge_l, 0.01)) * 100\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"‚úÖ {exp['name']} COMPLETED!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Train Loss:   {train_loss:.4f}\")\n",
    "        print(f\"Val Loss:     {val_loss:.4f}\")\n",
    "        print(f\"Perplexity:   {val_ppl:.2f}\")\n",
    "        print(f\"BLEU Score:   {val_bleu:.2f}%\")\n",
    "        print(f\"ROUGE-L Score: {val_rouge_l:.2f}%\")\n",
    "        print(f\"\\nImprovement over Baseline:\")\n",
    "        print(f\"  Loss:       {loss_imp:+.2f}%\")\n",
    "        print(f\"  Perplexity: {ppl_imp:+.2f}%\")\n",
    "        print(f\"  BLEU:       {bleu_imp:+.2f}%\")\n",
    "        print(f\"  ROUGE-L:    {rouge_imp:+.2f}%\")\n",
    "        print(f\"\\nResources:\")\n",
    "        print(f\"  GPU Memory: {gpu_mem_gb:.2f} GB (peak)\")\n",
    "        print(f\"  Time:       {train_time:.2f} minutes\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Experiment\": exp['name'],\n",
    "            \"Learning_Rate\": exp['lr'],\n",
    "            \"Batch_Size\": exp['batch'],\n",
    "            \"Grad_Accum\": exp['accum'],\n",
    "            \"Effective_Batch\": exp['batch'] * exp['accum'],\n",
    "            \"Epochs\": exp['epochs'],\n",
    "            \"Train_Loss\": round(train_loss, 4),\n",
    "            \"Val_Loss\": round(val_loss, 4),\n",
    "            \"Perplexity\": round(val_ppl, 2),\n",
    "            \"BLEU_Score\": round(val_bleu, 2),\n",
    "            \"ROUGE_L_Score\": round(val_rouge_l, 2),\n",
    "            \"Loss_Improvement_%\": round(loss_imp, 2),\n",
    "            \"Perplexity_Improvement_%\": round(ppl_imp, 2),\n",
    "            \"BLEU_Score\": round(val_bleu, 2),\n",
    "            \"ROUGE_L_Score\": round(val_rouge_l, 2),\n",
    "            \"BLEU_Improvement_%\": round(bleu_imp, 2),\n",
    "            \"ROUGE_L_Score\": round(val_rouge_l, 2),\n",
    "            \"ROUGE_L_Improvement_%\": round(rouge_imp, 2),\n",
    "            \"GPU_Memory_GB\": round(gpu_mem_gb, 2),\n",
    "            \"Time_Min\": round(train_time, 2)\n",
    "        })\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = f\"./best_{exp['name']}\"\n",
    "            print(f\"\\nüíæ Saving best model to {best_model_path}...\")\n",
    "            trainer.save_model(best_model_path)\n",
    "            tokenizer.save_pretrained(best_model_path)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"\\n‚ö†Ô∏è OOM ERROR in {exp['name']}\")\n",
    "            print(\"Try reducing MAX_SAMPLES further or using shorter sequences\")\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # Cleanup\n",
    "    del trainer, model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HvJ1w_3RRpF"
   },
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:56:34.842847Z",
     "iopub.status.busy": "2026-02-18T22:56:34.842535Z",
     "iopub.status.idle": "2026-02-18T22:56:34.868451Z",
     "shell.execute_reply": "2026-02-18T22:56:34.867858Z",
     "shell.execute_reply.started": "2026-02-18T22:56:34.842813Z"
    },
    "id": "GPMGDRP-RRpG",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "   Experiment Learning_Rate Batch_Size Grad_Accum Effective_Batch Epochs Train_Loss  Val_Loss  Perplexity  BLEU_Score  ROUGE_L_Score  Loss_Improvement_%  Perplexity_Improvement_%  BLEU_Improvement_%  ROUGE_L_Improvement_% GPU_Memory_GB Time_Min\n",
      "   ‚≠ê BASELINE             -          -          -               -      -          -   11.8996   147204.22       67.67          76.57                0.00                       0.0                0.00                   0.00             -        -\n",
      "  Exp1_HighLR        0.0001          2          4               8      5     0.5437    0.3521        1.42       89.03          95.11               97.04                     100.0               31.56                  24.21          1.52    13.56\n",
      "Exp2_MediumLR       0.00005          1          3               3      5     0.5717    0.3658        1.44       88.69          94.81               96.93                     100.0               31.06                  23.83          6.28     47.7\n",
      "   Exp3_LowLR       0.00002          2          2               4      5     0.5902    0.3791        1.46       74.85          80.96               96.81                     100.0               10.61                   5.74          2.83    14.43\n",
      "\n",
      "‚úÖ Results saved to: experiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "if len(results) == 0:\n",
    "    print(\"‚ö†Ô∏è No experiments completed successfully\")\n",
    "else:\n",
    "    print(\"\\nüìä RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Add baseline row\n",
    "    baseline_row = pd.DataFrame([{\n",
    "        \"Experiment\": \"‚≠ê BASELINE\",\n",
    "        \"Learning_Rate\": \"-\",\n",
    "        \"Batch_Size\": \"-\",\n",
    "        \"Grad_Accum\": \"-\",\n",
    "        \"Effective_Batch\": \"-\",\n",
    "        \"Epochs\": \"-\",\n",
    "        \"Train_Loss\": \"-\",\n",
    "        \"Val_Loss\": round(baseline_loss, 4),\n",
    "        \"Perplexity\": round(baseline_perplexity, 2),\n",
    "        \"BLEU_Score\": round(baseline_bleu, 2),\n",
    "        \"ROUGE_L_Score\": round(baseline_rouge_l, 2),\n",
    "        \"Loss_Improvement_%\": 0.0,\n",
    "        \"Perplexity_Improvement_%\": 0.0,\n",
    "        \"BLEU_Improvement_%\": 0.0,\n",
    "        \"ROUGE_L_Improvement_%\": 0.0,\n",
    "        \"GPU_Memory_GB\": \"-\",\n",
    "        \"Time_Min\": \"-\"\n",
    "    }])\n",
    "\n",
    "    full_results = pd.concat([baseline_row, results_df], ignore_index=True)\n",
    "\n",
    "    print(\"\\n\" + full_results.to_string(index=False))\n",
    "\n",
    "    # Save to CSV\n",
    "    full_results.to_csv(\"experiment_results.csv\", index=False)\n",
    "    print(\"\\n‚úÖ Results saved to: experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:56:34.869670Z",
     "iopub.status.busy": "2026-02-18T22:56:34.869323Z",
     "iopub.status.idle": "2026-02-18T22:56:34.876086Z",
     "shell.execute_reply": "2026-02-18T22:56:34.875274Z",
     "shell.execute_reply.started": "2026-02-18T22:56:34.869644Z"
    },
    "id": "zhOPIm_URRpG",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üèÜ BEST MODEL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Best Experiment: Exp1_HighLR\n",
      "Val Loss: 0.3521\n",
      "Perplexity: 1.42\n",
      "Improvement: +97.04%\n",
      "Hyperparameters:\n",
      "  - Learning Rate: 0.0001\n",
      "  - Effective Batch: 8\n",
      "  - Epochs: 5\n",
      "\n",
      "‚úÖ SUCCESS: 97.04%\n"
     ]
    }
   ],
   "source": [
    "if len(results) > 0:\n",
    "    # Best model analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üèÜ BEST MODEL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    best = results_df.loc[results_df['Val_Loss'].idxmin()]\n",
    "    print(f\"\\nBest Experiment: {best['Experiment']}\")\n",
    "    print(f\"Val Loss: {best['Val_Loss']:.4f}\")\n",
    "    print(f\"Perplexity: {best['Perplexity']:.2f}\")\n",
    "    print(f\"Improvement: {best['Loss_Improvement_%']:+.2f}%\")\n",
    "    print(f\"Hyperparameters:\")\n",
    "    print(f\"  - Learning Rate: {best['Learning_Rate']}\")\n",
    "    print(f\"  - Effective Batch: {best['Effective_Batch']}\")\n",
    "    print(f\"  - Epochs: {best['Epochs']}\")\n",
    "\n",
    "    # Check improvement threshold\n",
    "    max_imp = results_df['Loss_Improvement_%'].max()\n",
    "    if max_imp >= 10:\n",
    "        print(f\"\\n‚úÖ SUCCESS: {max_imp:.2f}%\")\n",
    "    else:\n",
    "        print(f\"\\nüìù Note: {max_imp:.2f}% improvement achieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:56:34.877505Z",
     "iopub.status.busy": "2026-02-18T22:56:34.877154Z",
     "iopub.status.idle": "2026-02-18T22:56:34.904462Z",
     "shell.execute_reply": "2026-02-18T22:56:34.903763Z",
     "shell.execute_reply.started": "2026-02-18T22:56:34.877467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING BEST MODEL FOR DEPLOYMENT\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Best model copied to: ./best_model/\n",
      "   Source: ./best_Exp1_HighLR\n",
      "\n",
      "üìÅ Model files (9 files):\n",
      "   - README.md\n",
      "   - adapter_config.json\n",
      "   - adapter_model.safetensors\n",
      "   - chat_template.jinja\n",
      "   - special_tokens_map.json\n",
      "   - tokenizer.json\n",
      "   - tokenizer.model\n",
      "   - tokenizer_config.json\n",
      "   - training_args.bin\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING BEST MODEL FOR DEPLOYMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_model_path and os.path.exists(best_model_path):\n",
    "    # Create a clean 'best_model' directory\n",
    "    deployment_path = \"./best_model\"\n",
    "    \n",
    "    if os.path.exists(deployment_path):\n",
    "        shutil.rmtree(deployment_path)\n",
    "    \n",
    "    # Copy the best model to deployment directory\n",
    "    shutil.copytree(best_model_path, deployment_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Best model copied to: {deployment_path}/\")\n",
    "    print(f\"   Source: {best_model_path}\")\n",
    "    \n",
    "    # Verify files\n",
    "    files = os.listdir(deployment_path)\n",
    "    print(f\"\\nüìÅ Model files ({len(files)} files):\")\n",
    "    for file in sorted(files):\n",
    "        print(f\"   - {file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Testing\n",
    "\n",
    "### Why Qualitative Testing?\n",
    "\n",
    "Numbers alone don't tell the full story. We need to:\n",
    "1. See actual model responses\n",
    "2. Compare baseline vs fine-tuned outputs\n",
    "3. Evaluate medical accuracy and relevance\n",
    "\n",
    "### Test Questions:\n",
    "\n",
    "We'll test 5 medical questions and compare:\n",
    "- **Baseline Model**: Pre-trained (no medical fine-tuning)\n",
    "- **Fine-tuned Model**: The medically-trained model\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "- More specific medical terminology\n",
    "- More accurate clinical information\n",
    "- Better structured responses\n",
    "- Domain-appropriate language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:56:34.905603Z",
     "iopub.status.busy": "2026-02-18T22:56:34.905372Z",
     "iopub.status.idle": "2026-02-18T22:56:38.180673Z",
     "shell.execute_reply": "2026-02-18T22:56:38.179872Z",
     "shell.execute_reply.started": "2026-02-18T22:56:34.905580Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded: ./best_Exp1_HighLR\n"
     ]
    }
   ],
   "source": [
    "if best_model_path:\n",
    "    fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "        best_model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded: {best_model_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No best model found, using last trained model\")\n",
    "    fine_tuned_model = model\n",
    "\n",
    "# Also load baseline for comparison\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:56:38.182035Z",
     "iopub.status.busy": "2026-02-18T22:56:38.181694Z",
     "iopub.status.idle": "2026-02-18T22:56:38.185715Z",
     "shell.execute_reply": "2026-02-18T22:56:38.185113Z",
     "shell.execute_reply.started": "2026-02-18T22:56:38.182007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define test questions\n",
    "test_questions = [\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"How is hypertension treated?\",\n",
    "    \"What causes pneumonia?\",\n",
    "    \"Explain the difference between Type 1 and Type 2 diabetes.\",\n",
    "    \"What are the side effects of aspirin?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:56:38.186712Z",
     "iopub.status.busy": "2026-02-18T22:56:38.186502Z",
     "iopub.status.idle": "2026-02-18T22:56:38.199024Z",
     "shell.execute_reply": "2026-02-18T22:56:38.198347Z",
     "shell.execute_reply.started": "2026-02-18T22:56:38.186692Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Response generation function ready\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, question, max_tokens=100):\n",
    "    \"\"\"Generate response from a model\"\"\"\n",
    "    prompt = f\"<|user|>\\nAnswer this question truthfully\\n{question}\\n<|assistant|>\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract only the assistant's response\n",
    "    if \"<|assistant|>\" in response:\n",
    "        response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ Response generation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:56:38.200226Z",
     "iopub.status.busy": "2026-02-18T22:56:38.199913Z",
     "iopub.status.idle": "2026-02-18T22:57:03.451573Z",
     "shell.execute_reply": "2026-02-18T22:57:03.450954Z",
     "shell.execute_reply.started": "2026-02-18T22:56:38.200204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUALITATIVE EVALUATION: Baseline vs Fine-tuned\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 1: What are the symptoms of diabetes?\n",
      "================================================================================\n",
      "\n",
      " BASELINE (Pre-trained):\n",
      "The symptoms of diabetes can vary depending on the type of diabetes and the individual's overall health. However, here are some common symptoms:\n",
      "1. Blurred vision: Diabetes can cause blurred vision, especially in the early stages.\n",
      "2. Thirst: People with diabetes may experience increased thirst and urination, which can lead to dehydration.\n",
      "3. Dry mouth: Diabetes can cause\n",
      "\n",
      " FINE-TUNED (Medical):\n",
      "Diabetes mellitus is characterized by the presence of polyuria and polydipsia, which are the classic symptoms of the disease.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Question 2: How is hypertension treated?\n",
      "================================================================================\n",
      "\n",
      " BASELINE (Pre-trained):\n",
      "Hypertension is treated with medication, lifestyle changes, and sometimes surgery. Medication can help lower blood pressure by reducing the amount of blood that is pumped out of the heart. This can be done through medications such as blood pressure pills, anti-hypertensive medications, or medications that lower blood pressure through the kidneys. Lifestyle changes such as quitting smoking, losing weight, and maintaining a healthy diet can\n",
      "\n",
      " FINE-TUNED (Medical):\n",
      "Hypertension is treated with medication.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Question 3: What causes pneumonia?\n",
      "================================================================================\n",
      "\n",
      " BASELINE (Pre-trained):\n",
      "Pneumonia is caused by infection with bacteria, viruses, or fungi in the lungs. The bacteria that cause pneumonia are usually spread through the air, while viruses and fungi can be spread through contact with infected surfaces or droplets. The infection can also occur in people who have weakened immune systems or have underlying medical conditions that weaken their immune system.\n",
      "\n",
      " FINE-TUNED (Medical):\n",
      "Pneumonia is caused by bacteria.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Question 4: Explain the difference between Type 1 and Type 2 diabetes.\n",
      "================================================================================\n",
      "\n",
      " BASELINE (Pre-trained):\n",
      "Type 1 diabetes is a chronic metabolic disorder characterized by the absence or dysfunction of insulin-producing cells in the pancreas. This results in a lack of insulin, which leads to high blood sugar levels (hyperglycemia) and the development of complications such as blindness, kidney failure, and heart disease.\n",
      "On the other hand, Type 2 diabetes is a metabolic disorder character\n",
      "\n",
      " FINE-TUNED (Medical):\n",
      "Type 1 diabetes is characterized by autoimmune destruction of Œ≤ cells in the pancreas, while Type 2 diabetes is characterized by insulin resistance due to dysfunction of Œ≤ cells or other insulin-producing cells in the pancreas.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Question 5: What are the side effects of aspirin?\n",
      "================================================================================\n",
      "\n",
      " BASELINE (Pre-trained):\n",
      "Aspirin can cause the following side effects:\n",
      "\n",
      "1. Gastric bleeding: Aspirin can cause bleeding in the stomach or intestines, which can lead to severe pain, vomiting, and bleeding.\n",
      "\n",
      "2. Stomach ulcers: Aspirin can cause stomach ulcers, which are inflammation of the lining of the stomach or intestines.\n",
      "\n",
      "3. Heartburn: Asp\n",
      "\n",
      " FINE-TUNED (Medical):\n",
      "Aspirin can cause gastric ulcers and hemorrhage.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run qualitative comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUALITATIVE EVALUATION: Baseline vs Fine-tuned\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Baseline response\n",
    "    print(\"\\n BASELINE (Pre-trained):\")\n",
    "    baseline_response = generate_response(baseline_model, question)\n",
    "    print(baseline_response)\n",
    "    \n",
    "    # Fine-tuned response\n",
    "    print(\"\\n FINE-TUNED (Medical):\")\n",
    "    finetuned_response = generate_response(fine_tuned_model, question)\n",
    "    print(finetuned_response)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
