{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Medical LLM Fine-Tuning - Complete Project\n\n**Domain:** Healthcare/Medical Q&A  \n**Model:** TinyLlama-1.1B-Chat-v1.0  \n**Method:** LoRA (Low-Rank Adaptation)  \n**Dataset:** Medical Meadow Medical Flashcards  \n\n---\n\n## Project Overview\n\nThis notebook implements a **domain-specific medical assistant** by fine-tuning a Large Language Model (LLM) on medical question-answer pairs. The goal is to create a model that can:\n\n1. Answer medical questions accurately\n2. Provide domain-specific responses\n3. Demonstrate measurable improvement over the base model\n\n### Why This Matters:\n- Medical information requires accuracy and domain expertise\n- Pre-trained models lack specialized medical knowledge\n- Fine-tuning adapts the model to medical terminology and reasoning\n\n---\n\n## Methodology\n\n### 1. **Dataset:** \n- Medical Meadow Medical Flashcards (Hugging Face)\n- Covers diverse medical topics and terminology\n\n### 2. **Fine-tuning Approach:**\n- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning\n- Achieves similar results to full fine-tuning\n\n### 3. **Evaluation:**\n- **Quantitative**: Loss, Perplexity, BLEU, ROUGE scores\n- **Qualitative**: Compare responses before/after fine-tuning\n- **Baseline**: Evaluate pre-trained model first for comparison\n\n---","metadata":{}},{"cell_type":"markdown","source":"## Installation\n\nThe required libraries for LLM fine-tuning are installed:\n\n- **transformers**: Hugging Face library for LLMs\n- **datasets**: Loading and processing datasets\n- **peft**: Parameter-Efficient Fine-Tuning (LoRA)\n- **accelerate**: Distributed training and mixed precision\n- **evaluate**: Metrics (BLEU, ROUGE, etc.)\n- **sentencepiece**: Tokenization support","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets peft accelerate bitsandbytes evaluate sentencepiece","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E0BkcVYWRRo_","outputId":"548d96f3-4d8c-4c63-c712-fdf2ae712d7f","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:02:00.859706Z","iopub.execute_input":"2026-02-15T06:02:00.860463Z","iopub.status.idle":"2026-02-15T06:02:04.342196Z","shell.execute_reply.started":"2026-02-15T06:02:00.860429Z","shell.execute_reply":"2026-02-15T06:02:04.341269Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"## Import Libraries\n\nImport all necessary libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport time\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom datasets import load_dataset, DatasetDict, Dataset\nfrom transformers import AutoTokenizer,AutoModelForCausalLM,TrainingArguments,Trainer\nfrom peft import LoraConfig, get_peft_model\nimport shutil\n\n# Seeds\ntorch.manual_seed(42)\nnp.random.seed(42)\nsns.set_style(\"whitegrid\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T08:25:03.864653Z","iopub.execute_input":"2026-02-15T08:25:03.865398Z","iopub.status.idle":"2026-02-15T08:25:03.871182Z","shell.execute_reply.started":"2026-02-15T08:25:03.865371Z","shell.execute_reply":"2026-02-15T08:25:03.870474Z"}},"outputs":[],"execution_count":110},{"cell_type":"markdown","source":"## Data Loading\n\n### Dataset: Medical Meadow Medical Flashcards\n\nThis dataset contains medical question-answer pairs in the format:\n- **Instruction**: Task description (e.g., \"Answer this question truthfully\")\n- **Input**: Medical question\n- **Output**: Correct medical answer\n\nExplore the dataset structure and sample entries to understand the data format.","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\nprint(f\"Total: {len(dataset['train']):,}\")\n\n# Show samples\nfor i in range(2):\n    print(f\"\\nExample {i+1}:\")\n    print(f\"Q: {dataset['train'][i]['input'][:80]}...\")\n    print(f\"A: {dataset['train'][i]['output'][:80]}...\")","metadata":{"id":"iaovQZXnRRpA","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:02:11.887303Z","iopub.execute_input":"2026-02-15T06:02:11.887627Z","iopub.status.idle":"2026-02-15T06:02:12.514557Z","shell.execute_reply.started":"2026-02-15T06:02:11.887572Z","shell.execute_reply":"2026-02-15T06:02:12.513916Z"}},"outputs":[{"name":"stdout","text":"Total: 33,955\n\nExample 1:\nQ: What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ leve...\nA: Very low Mg2+ levels correspond to low PTH levels which in turn results in low C...\n\nExample 2:\nQ: What leads to genitourinary syndrome of menopause (atrophic vaginitis)?...\nA: Low estradiol production leads to genitourinary syndrome of menopause (atrophic ...\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"## Data Preprocessing\n\n### Why Preprocessing Matters:\n\nRaw data often contains:\n- Missing values\n- Duplicate entries\n- Inconsistent formatting\n\n### Preprocessing Pipeline:\n\n1. **Data Cleaning**: Remove null/empty entries\n2. **Deduplication**: Remove duplicate Q&A pairs\n3. **Size Limiting**: Use 1,200 samples (GPU memory constraint)\n4. **Train/Val/Test Split**: 80/10/10 split\n\n### Key Decisions:\n- **2,0000 samples**: Balances training quality with GPU memory limits\n- **80/10/10 split**: Standard ML practice for train/validation/test","metadata":{}},{"cell_type":"code","source":"# Data cleaning\ndef clean_dataset(example):\n    return (\n        example['instruction'] and example['input'] and example['output'] and\n        len(str(example['instruction']).strip()) > 0 and\n        len(str(example['input']).strip()) > 0 and\n        len(str(example['output']).strip()) > 0\n    )\n\ncleaned = dataset['train'].filter(clean_dataset)\nprint(f\"After cleaning: {len(cleaned):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:03:28.658819Z","iopub.execute_input":"2026-02-15T06:03:28.659542Z","iopub.status.idle":"2026-02-15T06:03:28.671031Z","shell.execute_reply.started":"2026-02-15T06:03:28.659502Z","shell.execute_reply":"2026-02-15T06:03:28.670345Z"}},"outputs":[{"name":"stdout","text":"After cleaning: 33,547\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"# Remove duplicates\ndf = pd.DataFrame(cleaned)\ndf = df.drop_duplicates(subset=['input', 'output'])\ncleaned = Dataset.from_pandas(df)\nprint(f\"After dedup: {len(cleaned):,}\")","metadata":{"id":"OdOWEyaLRRpB","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:04:02.391204Z","iopub.execute_input":"2026-02-15T06:04:02.391979Z","iopub.status.idle":"2026-02-15T06:04:04.514230Z","shell.execute_reply.started":"2026-02-15T06:04:02.391947Z","shell.execute_reply":"2026-02-15T06:04:04.513412Z"}},"outputs":[{"name":"stdout","text":"After dedup: 33,521\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"MAX_SAMPLES = 2000\nif len(cleaned) > MAX_SAMPLES:\n    cleaned = cleaned.select(range(MAX_SAMPLES))\n\nprint(f\"‚úÖ Using {len(cleaned):,} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:04:35.567489Z","iopub.execute_input":"2026-02-15T06:04:35.568192Z","iopub.status.idle":"2026-02-15T06:04:35.574757Z","shell.execute_reply.started":"2026-02-15T06:04:35.568162Z","shell.execute_reply":"2026-02-15T06:04:35.574052Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Using 2,000 samples\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"# Split: 80% train, 10% val, 10% test\ntrain_test = cleaned.train_test_split(test_size=0.2, seed=42)\nval_test = train_test['test'].train_test_split(test_size=0.5, seed=42)\n\ndataset_split = DatasetDict({\n    'train': train_test['train'],\n    'validation': val_test['train'],\n    'test': val_test['test']\n})\n\nprint(f\"Train: {len(dataset_split['train']):,}\")\nprint(f\"Val: {len(dataset_split['validation']):,}\")\nprint(f\"Test: {len(dataset_split['test']):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:04:57.750934Z","iopub.execute_input":"2026-02-15T06:04:57.751665Z","iopub.status.idle":"2026-02-15T06:04:57.766286Z","shell.execute_reply.started":"2026-02-15T06:04:57.751636Z","shell.execute_reply":"2026-02-15T06:04:57.765588Z"}},"outputs":[{"name":"stdout","text":"Train: 1,600\nVal: 200\nTest: 200\n","output_type":"stream"}],"execution_count":72},{"cell_type":"markdown","source":"## Model Loading\n\n### Model: TinyLlama-1.1B-Chat-v1.0\n\n**Why TinyLlama?**\n- Compact (1.1B parameters vs 7B+ for larger models)\n- Fits in free GPU memory\n- Fast training\n- Good balance of quality and efficiency\n\n**Loading Configuration:**\n- `torch_dtype=torch.float16`: Half precision (saves memory)\n- `device_map=\"auto\"`: Automatically distribute across available devices\n- `low_cpu_mem_usage=True`: Minimize CPU memory footprint","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:06:00.628374Z","iopub.execute_input":"2026-02-15T06:06:00.629035Z","iopub.status.idle":"2026-02-15T06:06:00.632438Z","shell.execute_reply.started":"2026-02-15T06:06:00.629005Z","shell.execute_reply":"2026-02-15T06:06:00.631645Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\nprint(f\"‚úÖ Tokenizer loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:06:52.330620Z","iopub.execute_input":"2026-02-15T06:06:52.331320Z","iopub.status.idle":"2026-02-15T06:06:52.604298Z","shell.execute_reply.started":"2026-02-15T06:06:52.331290Z","shell.execute_reply":"2026-02-15T06:06:52.603678Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Tokenizer loaded\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"# Model\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,  # FP16 for GPU\n    device_map=\"auto\",\n    trust_remote_code=True,\n    low_cpu_mem_usage=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:07:40.133479Z","iopub.execute_input":"2026-02-15T06:07:40.134114Z","iopub.status.idle":"2026-02-15T06:07:41.046869Z","shell.execute_reply.started":"2026-02-15T06:07:40.134085Z","shell.execute_reply":"2026-02-15T06:07:41.046289Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"print(f\"‚úÖ Model loaded: {model.num_parameters():,} params\")\nprint(f\"GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:07:46.888050Z","iopub.execute_input":"2026-02-15T06:07:46.888719Z","iopub.status.idle":"2026-02-15T06:07:46.894123Z","shell.execute_reply.started":"2026-02-15T06:07:46.888689Z","shell.execute_reply":"2026-02-15T06:07:46.893438Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Model loaded: 1,100,048,384 params\nGPU Memory: 4.19 GB\n","output_type":"stream"}],"execution_count":76},{"cell_type":"markdown","source":"## LoRA Configuration\n\n### What is LoRA?\n\n**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning technique:\n\n- Traditional fine-tuning: Updates ALL 1.1B parameters\n-  oRA: Updates only ~1% of parameters (via low-rank matrices)\n\n### LoRA Settings:\n\n```\nr=16              # Rank of update matrices (higher = more capacity)\nlora_alpha=32     # Scaling factor\ntarget_modules    # Which layers to adapt (attention layers)\nlora_dropout=0.05 # Regularization\n```\n\n### Memory Savings:\n- Full fine-tuning: ~20GB GPU memory\n- LoRA: ~6-8GB GPU memory","metadata":{}},{"cell_type":"code","source":"model.config.use_cache = False","metadata":{"id":"1PwhvkFOh9ic","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:11:01.477894Z","iopub.execute_input":"2026-02-15T06:11:01.478449Z","iopub.status.idle":"2026-02-15T06:11:01.482085Z","shell.execute_reply.started":"2026-02-15T06:11:01.478420Z","shell.execute_reply":"2026-02-15T06:11:01.481299Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:13:17.496193Z","iopub.execute_input":"2026-02-15T06:13:17.496758Z","iopub.status.idle":"2026-02-15T06:13:17.500633Z","shell.execute_reply.started":"2026-02-15T06:13:17.496728Z","shell.execute_reply":"2026-02-15T06:13:17.499882Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"model = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:13:42.425584Z","iopub.execute_input":"2026-02-15T06:13:42.426321Z","iopub.status.idle":"2026-02-15T06:13:42.575577Z","shell.execute_reply.started":"2026-02-15T06:13:42.426290Z","shell.execute_reply":"2026-02-15T06:13:42.575042Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"‚úÖ LoRA applied\")\nprint(f\"Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:14:04.569579Z","iopub.execute_input":"2026-02-15T06:14:04.570337Z","iopub.status.idle":"2026-02-15T06:14:04.579221Z","shell.execute_reply.started":"2026-02-15T06:14:04.570298Z","shell.execute_reply":"2026-02-15T06:14:04.578380Z"}},"outputs":[{"name":"stdout","text":"‚úÖ LoRA applied\nTrainable: 4,505,600 (0.41%)\n","output_type":"stream"}],"execution_count":81},{"cell_type":"markdown","source":"## Tokenization\n\n### What is Tokenization?\n\nTokenization converts text into numbers that the model can process. For example:\n\n```\n\"What is diabetes?\" ‚Üí [2385, 310, 652, 9790, 29973]\n```\n\n### Tokenization Strategy:\n\n1. **Format**: Combine instruction + question + answer into training format\n2. **Max Length**: 256 tokens (balance quality vs memory)\n3. **Padding**: Pad all sequences to same length for batching\n4. **Labels**: Copy input_ids for causal language modeling\n\n### Why 256 tokens?\n- Medical Q&A pairs are typically short\n- Longer sequences = more GPU memory\n- 256 tokens = good balance","metadata":{}},{"cell_type":"code","source":"def format_instruction(example):\n    instruction = example['instruction']\n    question = example['input']\n    answer = example['output']\n    prompt = f\"<|user|>\\n{instruction}\\n{question}\\n<|assistant|>\\n{answer}{tokenizer.eos_token}\"\n    return {\"text\": prompt}","metadata":{"id":"phwIDkt9h4Lg","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:16:18.566831Z","iopub.execute_input":"2026-02-15T06:16:18.567179Z","iopub.status.idle":"2026-02-15T06:16:18.571975Z","shell.execute_reply.started":"2026-02-15T06:16:18.567152Z","shell.execute_reply":"2026-02-15T06:16:18.571048Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"formatted = dataset_split.map(\n    format_instruction,\n    remove_columns=dataset_split['train'].column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:16:50.349842Z","iopub.execute_input":"2026-02-15T06:16:50.350405Z","iopub.status.idle":"2026-02-15T06:16:50.664907Z","shell.execute_reply.started":"2026-02-15T06:16:50.350375Z","shell.execute_reply":"2026-02-15T06:16:50.664193Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d51be640fe01409395f37169eab77b58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"180985f35cd847ac8cd4cb9ab5d2c338"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2db0575735ba4adbadac32f7f82f3316"}},"metadata":{}}],"execution_count":83},{"cell_type":"code","source":"def tokenize_function(examples):\n    result = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=256,\n        padding=\"max_length\"\n    )\n    result[\"labels\"] = result[\"input_ids\"][:]\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:18:38.145756Z","iopub.execute_input":"2026-02-15T06:18:38.146398Z","iopub.status.idle":"2026-02-15T06:18:38.150490Z","shell.execute_reply.started":"2026-02-15T06:18:38.146369Z","shell.execute_reply":"2026-02-15T06:18:38.149667Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"tokenized = formatted.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:19:08.022082Z","iopub.execute_input":"2026-02-15T06:19:08.022389Z","iopub.status.idle":"2026-02-15T06:19:08.600987Z","shell.execute_reply.started":"2026-02-15T06:19:08.022360Z","shell.execute_reply":"2026-02-15T06:19:08.600194Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"027394955b994a41bd8de2b70d7b2779"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a20f73f80d54852a6e24cff9f620679"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"969fd375466c45df96b5063ee3f86d8d"}},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"train_data = tokenized['train']\nval_data = tokenized['validation']\n\nprint(f\"‚úÖ Train: {len(train_data):,}, Val: {len(val_data):,}\")\nprint(f\"Max length: 256 tokens\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:19:31.139372Z","iopub.execute_input":"2026-02-15T06:19:31.140092Z","iopub.status.idle":"2026-02-15T06:19:31.145312Z","shell.execute_reply.started":"2026-02-15T06:19:31.140053Z","shell.execute_reply":"2026-02-15T06:19:31.144405Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Train: 1,600, Val: 200\nMax length: 256 tokens\n","output_type":"stream"}],"execution_count":86},{"cell_type":"markdown","source":"## Evaluation Metrics Setup\n\n### Metrics that are used:\n\n1. **Loss**: How well the model predicts next tokens (lower is better)\n2. **Perplexity**: Exp(loss), measures uncertainty (lower is better)\n3. **BLEU**: Measures n-gram overlap with reference (0-100, higher is better)\n4. **ROUGE**: Measures recall of n-grams (0-1, higher is better)\n\n### Why Multiple Metrics?\n\n- **Loss/Perplexity**: Overall model quality\n- **BLEU**: Precision of generated text\n- **ROUGE**: Recall/coverage of key information","metadata":{}},{"cell_type":"code","source":"# Load evaluation metrics\nbleu_metric = evaluate.load(\"bleu\")\nrouge_metric = evaluate.load(\"rouge\")\n\ndef compute_perplexity(loss):\n    return np.exp(loss)\n\nprint(\"‚úÖ Metrics configured (loss, perplexity, BLEU & ROUGE)\")","metadata":{"id":"T3JKL5JQRRpE","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:20:42.189076Z","iopub.execute_input":"2026-02-15T06:20:42.189654Z","iopub.status.idle":"2026-02-15T06:20:44.027592Z","shell.execute_reply.started":"2026-02-15T06:20:42.189612Z","shell.execute_reply":"2026-02-15T06:20:44.026668Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0a048b78fc549b4bc48e1766fd2efd9"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Metrics configured (loss, perplexity, BLEU & ROUGE)\n","output_type":"stream"}],"execution_count":87},{"cell_type":"markdown","source":"## Baseline evaluation","metadata":{"id":"8OBGeWo5jGKN"}},{"cell_type":"code","source":"# Clean memory\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"id":"fgmRh1BBjN-L","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:21:00.916672Z","iopub.execute_input":"2026-02-15T06:21:00.918320Z","iopub.status.idle":"2026-02-15T06:21:01.520437Z","shell.execute_reply.started":"2026-02-15T06:21:00.918280Z","shell.execute_reply":"2026-02-15T06:21:01.519761Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"baseline_args = TrainingArguments(\n    output_dir=\"./baseline\",\n    per_device_eval_batch_size=1,\n    fp16=True,\n    report_to=\"none\",\n    prediction_loss_only=True,\n    dataloader_num_workers=0,  )","metadata":{"id":"5f8hlhRojQnL","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:21:03.909530Z","iopub.execute_input":"2026-02-15T06:21:03.910263Z","iopub.status.idle":"2026-02-15T06:21:03.946205Z","shell.execute_reply.started":"2026-02-15T06:21:03.910235Z","shell.execute_reply":"2026-02-15T06:21:03.945662Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"baseline_trainer = Trainer(\n    model=model,\n    args=baseline_args,\n    eval_dataset=val_data,\n)","metadata":{"id":"jDyoKclOjVEI","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:24:19.612361Z","iopub.execute_input":"2026-02-15T06:24:19.612692Z","iopub.status.idle":"2026-02-15T06:24:19.625048Z","shell.execute_reply.started":"2026-02-15T06:24:19.612666Z","shell.execute_reply":"2026-02-15T06:24:19.624397Z"}},"outputs":[{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"baseline_metrics = baseline_trainer.evaluate()\nbaseline_loss = baseline_metrics['eval_loss']\nbaseline_perplexity = compute_perplexity(baseline_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:21:21.268217Z","iopub.execute_input":"2026-02-15T06:21:21.268737Z","iopub.status.idle":"2026-02-15T06:21:51.728046Z","shell.execute_reply.started":"2026-02-15T06:21:21.268710Z","shell.execute_reply":"2026-02-15T06:21:51.727275Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 00:30]\n    </div>\n    "},"metadata":{}}],"execution_count":91},{"cell_type":"code","source":"baseline_predictions = []\nbaseline_references = []\nmodel.eval()\nwith torch.no_grad():\n    for i in range(min(50, len(val_data))):  # Sample 50 for speed\n        sample = val_data[i]\n        input_ids = torch.tensor([sample[\"input_ids\"][:128]]).to(model.device)\n        attention_mask = torch.tensor([[1] * len(sample[\"input_ids\"][:128])]).to(model.device)\n        outputs = model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=50,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        ref_text = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n        baseline_predictions.append(pred_text)\n        baseline_references.append([ref_text])\n","metadata":{"id":"HG08yUgtjckb"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline_bleu_result = bleu_metric.compute(predictions=baseline_predictions, references=baseline_references)\nbaseline_bleu = baseline_bleu_result[\"bleu\"] * 100  # Convert to percentage\n\n# Calculate ROUGE scores\nbaseline_rouge_result = rouge_metric.compute(predictions=baseline_predictions, references=baseline_references)\nbaseline_rouge_l = baseline_rouge_result[\"rougeL\"] * 100  # ROUGE-L F1 score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:22:09.287997Z","iopub.execute_input":"2026-02-15T06:22:09.288751Z","iopub.status.idle":"2026-02-15T06:22:09.613063Z","shell.execute_reply.started":"2026-02-15T06:22:09.288717Z","shell.execute_reply":"2026-02-15T06:22:09.612513Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"\nprint(\"BASELINE METRICS\")\nprint(\"=\"*80)\nprint(f\"   Loss: {baseline_loss:.4f}\")\nprint(f\"   Perplexity: {baseline_perplexity:.2f}\")\nprint(f\"   BLEU: {baseline_bleu:.2f}\")\nprint(f\"   ROGUE: {baseline_rouge_l:.2f}\")\nprint(\"=\"*80)","metadata":{"id":"fZwcNXdqjfqL","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:24:05.807943Z","iopub.execute_input":"2026-02-15T06:24:05.808477Z","iopub.status.idle":"2026-02-15T06:24:05.813063Z","shell.execute_reply.started":"2026-02-15T06:24:05.808446Z","shell.execute_reply":"2026-02-15T06:24:05.812347Z"}},"outputs":[{"name":"stdout","text":"BASELINE METRICS\n================================================================================\n   Loss: 11.8996\n   Perplexity: 147204.22\n   BLEU: 67.67\n   ROGUE: 76.57\n================================================================================\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"# Cleanup\ndel baseline_trainer\ngc.collect()\ntorch.cuda.empty_cache()\n\n","metadata":{"id":"Qx6EUuS1RRpE","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:24:28.286379Z","iopub.execute_input":"2026-02-15T06:24:28.286717Z","iopub.status.idle":"2026-02-15T06:24:28.684026Z","shell.execute_reply.started":"2026-02-15T06:24:28.286689Z","shell.execute_reply":"2026-02-15T06:24:28.683389Z"}},"outputs":[],"execution_count":99},{"cell_type":"markdown","source":"## Hyperparameter Experiments\n\n\n\n### Experimental Design:\n\n3 different configurations are tested to find the optimal hyperparameters:\n\n| Experiment | Learning Rate | Batch Size | Grad Accum | Effective Batch | Epochs | Warmup |\n|------------|--------------|------------|------------|-----------------|--------|--------|\n| **Exp1 (High LR)** | 1e-4 | 2 | 4 | 8 | 5 | 50 |\n| **Exp2 (Medium LR)** | 5e-5 | 1 | 3 | 3 | 5 | 50 |\n| **Exp3 (Low LR)** | 2e-5 | 2 | 2 | 4 | 5 | 50 |\n\n### Hyperparameter Explanations:\n\n**Learning Rate (lr):**\n- Controls how much the model updates weights each step\n- **High (1e-4)**: Faster learning but risk of instability\n- **Medium (5e-5)**: Balanced approach (often optimal)\n- **Low (2e-5)**: Slower but more stable convergence\n\n**Batch Size:**\n- Number of samples processed together\n- Smaller batches = noisier but more frequent updates\n\n**Gradient Accumulation (accum):**\n- Accumulates gradients over multiple batches before updating\n- **Effective Batch = Batch Size √ó Accumulation**\n- Simulates larger batches without memory overflow\n\n**Epochs:**\n- Number of times model sees entire dataset\n- **5 epochs**: Sufficient for convergence with our dataset size\n- More epochs = better learning but risk of overfitting\n\n**Warmup Steps:**\n- Gradually increases learning rate at start\n- **50 steps**: Prevents instability in early training\n- Helps model adjust smoothly to the data\n\n### What is tracked for Each Experiment:\n\n1. **Training Loss**: How well the model learns the training data\n2. **Validation Loss**: Performance on unseen data (generalization)\n3. **BLEU Score**: Quality of generated medical responses (0-100%)\n4. **ROUGE-L Score**: Coverage of key information (0-100%)\n5. **Perplexity**: Model confidence (lower = better)\n6. **Improvement %**: Gain over baseline model\n7. **GPU Memory**: Peak memory usage in GB\n8. **Training Time**: Minutes to complete\n\n### Why This Matters:\n\nBy comparing results across experiments, we can:\n- Identify the best learning rate for our task\n- Understand hyperparameter impact on performance\n- Demonstrate systematic optimization approach\n- Select the optimal configuration for deployment","metadata":{}},{"cell_type":"code","source":"experiments = [\n    {\"name\": \"Exp1_HighLR\", \"lr\": 1e-4, \"batch\": 2, \"accum\": 4, \"epochs\": 5, \"warmup\": 50},\n    {\"name\": \"Exp2_MediumLR\", \"lr\": 5e-5, \"batch\": 1, \"accum\": 3, \"epochs\": 5, \"warmup\": 50},\n    {\"name\": \"Exp3_LowLR\", \"lr\": 2e-5, \"batch\": 2, \"accum\": 2, \"epochs\": 5, \"warmup\": 50},\n]\n\n","metadata":{"id":"A8kmHmyqRRpE","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:29:01.072325Z","iopub.execute_input":"2026-02-15T06:29:01.073027Z","iopub.status.idle":"2026-02-15T06:29:01.077197Z","shell.execute_reply.started":"2026-02-15T06:29:01.072997Z","shell.execute_reply":"2026-02-15T06:29:01.076380Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"results = []\nbest_model_path = None\nbest_val_loss = float('inf')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:29:03.011396Z","iopub.execute_input":"2026-02-15T06:29:03.012026Z","iopub.status.idle":"2026-02-15T06:29:03.015333Z","shell.execute_reply.started":"2026-02-15T06:29:03.011998Z","shell.execute_reply":"2026-02-15T06:29:03.014675Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"for idx, exp in enumerate(experiments, 1):\n\n    print(f\"üß™ EXPERIMENT {idx}/{len(experiments)}: {exp['name']}\")\n    print(\"=\"*80)\n    print(f\"LR: {exp['lr']}, Batch: {exp['batch']}, Accum: {exp['accum']}, Epochs: {exp['epochs']}\")\n    print(f\"Effective batch size: {exp['batch'] * exp['accum']}\")\n    print(\"-\"*80)\n\n    # Reload model fresh\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n    )\n    model.config.use_cache = False\n    \n    # CRITICAL: Disable gradient checkpointing before LoRA (Kaggle fix)\n    if hasattr(model, \"gradient_checkpointing_disable\"):\n        model.gradient_checkpointing_disable()\n    model = get_peft_model(model, lora_config)\n    \n    # Ensure LoRA parameters require gradients\n    for name, param in model.named_parameters():\n        if \"lora\" in name.lower():\n            param.requires_grad = True\n    \n    # Verify trainable parameters\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    if trainable_params == 0:\n        print(\"‚ö†Ô∏è WARNING: No trainable parameters! Skipping this experiment.\")\n        continue\n\n    # Training arguments - GPU optimized\n    training_args = TrainingArguments(\n        output_dir=f\"./results_{exp['name']}\",\n        per_device_train_batch_size=exp['batch'],\n        per_device_eval_batch_size=1,\n        gradient_accumulation_steps=exp['accum'],\n        num_train_epochs=exp['epochs'],\n        learning_rate=exp['lr'],\n        weight_decay=0.01,\n        warmup_steps=exp['warmup'],\n        lr_scheduler_type=\"cosine\",\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_steps=50,\n        fp16=True,  # FP16 for GPU\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        report_to=\"none\",\n        save_total_limit=1,\n        prediction_loss_only=True,  # CRITICAL for memory!\n        dataloader_num_workers=0,\n        # gradient_checkpointing=True,  # Disabled for Kaggle compatibility\n    )\n\n    # Clean memory\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_data,\n        eval_dataset=val_data,\n    )\n\n    print(f\"\\nüöÄ Training...\")\n    start = time.time()\n\n    try:\n        train_result = trainer.train()\n        train_time = (time.time() - start) / 60\n\n        print(f\"\\nüìä Evaluating...\")\n        eval_metrics = trainer.evaluate()\n\n        # Extract metrics\n        train_loss = train_result.training_loss\n        val_loss = eval_metrics['eval_loss']\n        val_ppl = compute_perplexity(val_loss)\n\n        # Calculate BLEU score\n        print(\"Calculating BLEU score...\")\n        predictions = []\n        references = []\n        model.eval()\n        with torch.no_grad():\n            for i in range(min(50, len(val_data))):\n                sample = val_data[i]\n                input_ids = torch.tensor([sample[\"input_ids\"][:128]]).to(model.device)\n                outputs = model.generate(\n                    input_ids,\n                    attention_mask=attention_mask,\n                    max_new_tokens=50,\n                    do_sample=False,\n                    pad_token_id=tokenizer.eos_token_id\n                )\n                pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n                ref_text = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n                predictions.append(pred_text)\n                references.append([ref_text])\n\n        bleu_result = bleu_metric.compute(predictions=predictions, references=references)\n        val_bleu = bleu_result[\"bleu\"] * 100\n\n        # Calculate ROUGE score\n        rouge_result = rouge_metric.compute(predictions=predictions, references=references)\n        val_rouge_l = rouge_result[\"rougeL\"] * 100\n        gpu_mem_gb = torch.cuda.max_memory_allocated() / (1024**3)\n\n        # Calculate improvements\n        loss_imp = ((baseline_loss - val_loss) / baseline_loss) * 100\n        ppl_imp = ((baseline_perplexity - val_ppl) / baseline_perplexity) * 100\n        bleu_imp = ((val_bleu - baseline_bleu) / max(baseline_bleu, 0.01)) * 100\n        rouge_imp = ((val_rouge_l - baseline_rouge_l) / max(baseline_rouge_l, 0.01)) * 100\n\n        # Display results\n        print(\"\\n\" + \"=\"*80)\n        print(f\"‚úÖ {exp['name']} COMPLETED!\")\n        print(\"=\"*80)\n        print(f\"Train Loss:   {train_loss:.4f}\")\n        print(f\"Val Loss:     {val_loss:.4f}\")\n        print(f\"Perplexity:   {val_ppl:.2f}\")\n        print(f\"BLEU Score:   {val_bleu:.2f}%\")\n        print(f\"ROUGE-L Score: {val_rouge_l:.2f}%\")\n        print(f\"\\nImprovement over Baseline:\")\n        print(f\"  Loss:       {loss_imp:+.2f}%\")\n        print(f\"  Perplexity: {ppl_imp:+.2f}%\")\n        print(f\"  BLEU:       {bleu_imp:+.2f}%\")\n        print(f\"  ROUGE-L:    {rouge_imp:+.2f}%\")\n        print(f\"\\nResources:\")\n        print(f\"  GPU Memory: {gpu_mem_gb:.2f} GB (peak)\")\n        print(f\"  Time:       {train_time:.2f} minutes\")\n        print(\"=\"*80)\n\n        # Store results\n        results.append({\n            \"Experiment\": exp['name'],\n            \"Learning_Rate\": exp['lr'],\n            \"Batch_Size\": exp['batch'],\n            \"Grad_Accum\": exp['accum'],\n            \"Effective_Batch\": exp['batch'] * exp['accum'],\n            \"Epochs\": exp['epochs'],\n            \"Train_Loss\": round(train_loss, 4),\n            \"Val_Loss\": round(val_loss, 4),\n            \"Perplexity\": round(val_ppl, 2),\n            \"BLEU_Score\": round(val_bleu, 2),\n            \"ROUGE_L_Score\": round(val_rouge_l, 2),\n            \"Loss_Improvement_%\": round(loss_imp, 2),\n            \"Perplexity_Improvement_%\": round(ppl_imp, 2),\n            \"BLEU_Score\": round(val_bleu, 2),\n            \"ROUGE_L_Score\": round(val_rouge_l, 2),\n            \"BLEU_Improvement_%\": round(bleu_imp, 2),\n            \"ROUGE_L_Score\": round(val_rouge_l, 2),\n            \"ROUGE_L_Improvement_%\": round(rouge_imp, 2),\n            \"GPU_Memory_GB\": round(gpu_mem_gb, 2),\n            \"Time_Min\": round(train_time, 2)\n        })\n\n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_path = f\"./best_{exp['name']}\"\n            print(f\"\\nüíæ Saving best model to {best_model_path}...\")\n            trainer.save_model(best_model_path)\n            tokenizer.save_pretrained(best_model_path)\n\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            print(f\"\\n‚ö†Ô∏è OOM ERROR in {exp['name']}\")\n            print(\"Try reducing MAX_SAMPLES further or using shorter sequences\")\n            gc.collect()\n            torch.cuda.empty_cache()\n            continue\n        else:\n            raise e\n\n    # Cleanup\n    del trainer, model\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"id":"Ue0Y2yUERRpF","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T06:29:26.042215Z","iopub.execute_input":"2026-02-15T06:29:26.042935Z","iopub.status.idle":"2026-02-15T08:14:53.816461Z","shell.execute_reply.started":"2026-02-15T06:29:26.042904Z","shell.execute_reply":"2026-02-15T08:14:53.815638Z"}},"outputs":[{"name":"stdout","text":"üß™ EXPERIMENT 1/3: Exp1_HighLR\n================================================================================\nLR: 0.0001, Batch: 2, Accum: 4, Epochs: 5\nEffective batch size: 8\n--------------------------------------------------------------------------------\nTrainable parameters: 4,505,600\n","output_type":"stream"},{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\n","output_type":"stream"},{"name":"stdout","text":"\nüöÄ Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 27:00, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.393500</td>\n      <td>0.401371</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.364300</td>\n      <td>0.382663</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.344200</td>\n      <td>0.372744</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.332700</td>\n      <td>0.368137</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.325200</td>\n      <td>0.367994</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nüìä Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 00:30]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Calculating BLEU score...\n\n================================================================================\n‚úÖ Exp1_HighLR COMPLETED!\n================================================================================\nTrain Loss:   0.7976\nVal Loss:     0.3680\nPerplexity:   1.44\nBLEU Score:   88.67%\nROUGE-L Score: 94.72%\n\nImprovement over Baseline:\n  Loss:       +96.91%\n  Perplexity: +100.00%\n  BLEU:       +31.04%\n  ROUGE-L:    +23.71%\n\nResources:\n  GPU Memory: 6.28 GB (peak)\n  Time:       27.06 minutes\n================================================================================\n\nüíæ Saving best model to ./best_Exp1_HighLR...\nüß™ EXPERIMENT 2/3: Exp2_MediumLR\n================================================================================\nLR: 5e-05, Batch: 1, Accum: 3, Epochs: 5\nEffective batch size: 3\n--------------------------------------------------------------------------------\nTrainable parameters: 4,505,600\n","output_type":"stream"},{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\n","output_type":"stream"},{"name":"stdout","text":"\nüöÄ Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1335' max='1335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1335/1335 47:55, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.371100</td>\n      <td>0.392259</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.349500</td>\n      <td>0.377172</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.327500</td>\n      <td>0.368737</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.324500</td>\n      <td>0.365832</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.319800</td>\n      <td>0.365865</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nüìä Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 00:30]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Calculating BLEU score...\n\n================================================================================\n‚úÖ Exp2_MediumLR COMPLETED!\n================================================================================\nTrain Loss:   0.5717\nVal Loss:     0.3658\nPerplexity:   1.44\nBLEU Score:   88.69%\nROUGE-L Score: 94.81%\n\nImprovement over Baseline:\n  Loss:       +96.93%\n  Perplexity: +100.00%\n  BLEU:       +31.06%\n  ROUGE-L:    +23.83%\n\nResources:\n  GPU Memory: 6.28 GB (peak)\n  Time:       47.97 minutes\n================================================================================\n\nüíæ Saving best model to ./best_Exp2_MediumLR...\nüß™ EXPERIMENT 3/3: Exp3_LowLR\n================================================================================\nLR: 2e-05, Batch: 2, Accum: 2, Epochs: 5\nEffective batch size: 4\n--------------------------------------------------------------------------------\nTrainable parameters: 4,505,600\n","output_type":"stream"},{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\n","output_type":"stream"},{"name":"stdout","text":"\nüöÄ Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 27:15, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.394000</td>\n      <td>0.411925</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.381300</td>\n      <td>0.397659</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.368600</td>\n      <td>0.391873</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.364800</td>\n      <td>0.389704</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.368300</td>\n      <td>0.389286</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nüìä Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 00:30]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Calculating BLEU score...\n\n================================================================================\n‚úÖ Exp3_LowLR COMPLETED!\n================================================================================\nTrain Loss:   0.8304\nVal Loss:     0.3893\nPerplexity:   1.48\nBLEU Score:   80.38%\nROUGE-L Score: 85.27%\n\nImprovement over Baseline:\n  Loss:       +96.73%\n  Perplexity: +100.00%\n  BLEU:       +18.78%\n  ROUGE-L:    +11.37%\n\nResources:\n  GPU Memory: 6.28 GB (peak)\n  Time:       27.29 minutes\n================================================================================\n","output_type":"stream"}],"execution_count":102},{"cell_type":"markdown","source":"## Results Analysis","metadata":{"id":"7HvJ1w_3RRpF"}},{"cell_type":"code","source":"if len(results) == 0:\n    print(\"‚ö†Ô∏è No experiments completed successfully\")\nelse:\n    print(\"\\nüìä RESULTS SUMMARY\")\n    print(\"=\"*80)\n\n    results_df = pd.DataFrame(results)\n\n    # Add baseline row\n    baseline_row = pd.DataFrame([{\n        \"Experiment\": \"‚≠ê BASELINE\",\n        \"Learning_Rate\": \"-\",\n        \"Batch_Size\": \"-\",\n        \"Grad_Accum\": \"-\",\n        \"Effective_Batch\": \"-\",\n        \"Epochs\": \"-\",\n        \"Train_Loss\": \"-\",\n        \"Val_Loss\": round(baseline_loss, 4),\n        \"Perplexity\": round(baseline_perplexity, 2),\n        \"BLEU_Score\": round(baseline_bleu, 2),\n        \"ROUGE_L_Score\": round(baseline_rouge_l, 2),\n        \"Loss_Improvement_%\": 0.0,\n        \"Perplexity_Improvement_%\": 0.0,\n        \"BLEU_Improvement_%\": 0.0,\n        \"ROUGE_L_Improvement_%\": 0.0,\n        \"GPU_Memory_GB\": \"-\",\n        \"Time_Min\": \"-\"\n    }])\n\n    full_results = pd.concat([baseline_row, results_df], ignore_index=True)\n\n    print(\"\\n\" + full_results.to_string(index=False))\n\n    # Save to CSV\n    full_results.to_csv(\"experiment_results.csv\", index=False)\n    print(\"\\n‚úÖ Results saved to: experiment_results.csv\")","metadata":{"id":"GPMGDRP-RRpG","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T08:14:53.817937Z","iopub.execute_input":"2026-02-15T08:14:53.818548Z","iopub.status.idle":"2026-02-15T08:14:53.844870Z","shell.execute_reply.started":"2026-02-15T08:14:53.818519Z","shell.execute_reply":"2026-02-15T08:14:53.844275Z"}},"outputs":[{"name":"stdout","text":"\nüìä RESULTS SUMMARY\n================================================================================\n\n   Experiment Learning_Rate Batch_Size Grad_Accum Effective_Batch Epochs Train_Loss  Val_Loss  Perplexity  BLEU_Score  ROUGE_L_Score  Loss_Improvement_%  Perplexity_Improvement_%  BLEU_Improvement_%  ROUGE_L_Improvement_% GPU_Memory_GB Time_Min\n   ‚≠ê BASELINE             -          -          -               -      -          -   11.8996   147204.22       67.67          76.57                0.00                       0.0                0.00                   0.00             -        -\n  Exp1_HighLR        0.0001          2          4               8      5     0.7976    0.3680        1.44       88.67          94.72               96.91                     100.0               31.04                  23.71          6.28    27.06\nExp2_MediumLR       0.00005          1          3               3      5     0.5717    0.3658        1.44       88.69          94.81               96.93                     100.0               31.06                  23.83          6.28    47.97\n   Exp3_LowLR       0.00002          2          2               4      5     0.8304    0.3893        1.48       80.38          85.27               96.73                     100.0               18.78                  11.37          6.28    27.29\n\n‚úÖ Results saved to: experiment_results.csv\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"if len(results) > 0:\n    # Best model analysis\n    print(\"\\n\" + \"=\"*80)\n    print(\"üèÜ BEST MODEL ANALYSIS\")\n    print(\"=\"*80)\n\n    best = results_df.loc[results_df['Val_Loss'].idxmin()]\n    print(f\"\\nBest Experiment: {best['Experiment']}\")\n    print(f\"Val Loss: {best['Val_Loss']:.4f}\")\n    print(f\"Perplexity: {best['Perplexity']:.2f}\")\n    print(f\"Improvement: {best['Loss_Improvement_%']:+.2f}%\")\n    print(f\"Hyperparameters:\")\n    print(f\"  - Learning Rate: {best['Learning_Rate']}\")\n    print(f\"  - Effective Batch: {best['Effective_Batch']}\")\n    print(f\"  - Epochs: {best['Epochs']}\")\n\n    # Check improvement threshold\n    max_imp = results_df['Loss_Improvement_%'].max()\n    if max_imp >= 10:\n        print(f\"\\n‚úÖ SUCCESS: {max_imp:.2f}%\")\n    else:\n        print(f\"\\nüìù Note: {max_imp:.2f}% improvement achieved\")","metadata":{"id":"zhOPIm_URRpG","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T08:15:28.742137Z","iopub.execute_input":"2026-02-15T08:15:28.742458Z","iopub.status.idle":"2026-02-15T08:15:28.749453Z","shell.execute_reply.started":"2026-02-15T08:15:28.742432Z","shell.execute_reply":"2026-02-15T08:15:28.748619Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüèÜ BEST MODEL ANALYSIS\n================================================================================\n\nBest Experiment: Exp2_MediumLR\nVal Loss: 0.3658\nPerplexity: 1.44\nImprovement: +96.93%\nHyperparameters:\n  - Learning Rate: 5e-05\n  - Effective Batch: 3\n  - Epochs: 5\n\n‚úÖ SUCCESS: 96.93%\n","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"SAVING BEST MODEL FOR DEPLOYMENT\")\nprint(\"=\"*80)\n\nif best_model_path and os.path.exists(best_model_path):\n    # Create a clean 'best_model' directory\n    deployment_path = \"./best_model\"\n    \n    if os.path.exists(deployment_path):\n        shutil.rmtree(deployment_path)\n    \n    # Copy the best model to deployment directory\n    shutil.copytree(best_model_path, deployment_path)\n    \n    print(f\"\\n‚úÖ Best model copied to: {deployment_path}/\")\n    print(f\"   Source: {best_model_path}\")\n    \n    # Verify files\n    files = os.listdir(deployment_path)\n    print(f\"\\nüìÅ Model files ({len(files)} files):\")\n    for file in sorted(files):\n        print(f\"   - {file}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T08:26:11.064015Z","iopub.execute_input":"2026-02-15T08:26:11.064342Z","iopub.status.idle":"2026-02-15T08:26:11.087889Z","shell.execute_reply.started":"2026-02-15T08:26:11.064308Z","shell.execute_reply":"2026-02-15T08:26:11.087094Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSAVING BEST MODEL FOR DEPLOYMENT\n================================================================================\n\n‚úÖ Best model copied to: ./best_model/\n   Source: ./best_Exp2_MediumLR\n\nüìÅ Model files (9 files):\n   - README.md\n   - adapter_config.json\n   - adapter_model.safetensors\n   - chat_template.jinja\n   - special_tokens_map.json\n   - tokenizer.json\n   - tokenizer.model\n   - tokenizer_config.json\n   - training_args.bin\n","output_type":"stream"}],"execution_count":111},{"cell_type":"markdown","source":"## Qualitative Testing\n\n### Why Qualitative Testing?\n\nNumbers alone don't tell the full story. We need to:\n1. See actual model responses\n2. Compare baseline vs fine-tuned outputs\n3. Evaluate medical accuracy and relevance\n\n### Test Questions:\n\nWe'll test 5 medical questions and compare:\n- **Baseline Model**: Pre-trained (no medical fine-tuning)\n- **Fine-tuned Model**: The medically-trained model\n\n### What to Look For:\n\n- More specific medical terminology\n- More accurate clinical information\n- Better structured responses\n- Domain-appropriate language","metadata":{}},{"cell_type":"code","source":"if best_model_path:\n    fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n        best_model_path,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n    print(f\"‚úÖ Loaded: {best_model_path}\")\nelse:\n    print(\"‚ö†Ô∏è No best model found, using last trained model\")\n    fine_tuned_model = model\n\n# Also load baseline for comparison\nbaseline_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T08:30:31.997760Z","iopub.execute_input":"2026-02-15T08:30:31.998094Z","iopub.status.idle":"2026-02-15T08:30:35.301363Z","shell.execute_reply.started":"2026-02-15T08:30:31.998068Z","shell.execute_reply":"2026-02-15T08:30:35.300586Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Loaded: ./best_Exp2_MediumLR\n","output_type":"stream"}],"execution_count":113},{"cell_type":"code","source":"# Define test questions\ntest_questions = [\n    \"What are the symptoms of diabetes?\",\n    \"How is hypertension treated?\",\n    \"What causes pneumonia?\",\n    \"Explain the difference between Type 1 and Type 2 diabetes.\",\n    \"What are the side effects of aspirin?\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T08:30:58.729813Z","iopub.execute_input":"2026-02-15T08:30:58.730405Z","iopub.status.idle":"2026-02-15T08:30:58.733763Z","shell.execute_reply.started":"2026-02-15T08:30:58.730375Z","shell.execute_reply":"2026-02-15T08:30:58.733031Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"def generate_response(model, question, max_tokens=100):\n    \"\"\"Generate response from a model\"\"\"\n    prompt = f\"<|user|>\\nAnswer this question truthfully\\n{question}\\n<|assistant|>\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # Extract only the assistant's response\n    if \"<|assistant|>\" in response:\n        response = response.split(\"<|assistant|>\")[-1].strip()\n    \n    return response\n\nprint(\"‚úÖ Response generation function ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T08:31:05.199139Z","iopub.execute_input":"2026-02-15T08:31:05.199429Z","iopub.status.idle":"2026-02-15T08:31:05.205697Z","shell.execute_reply.started":"2026-02-15T08:31:05.199404Z","shell.execute_reply":"2026-02-15T08:31:05.204933Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Response generation function ready\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"# Run qualitative comparison\nprint(\"\\n\" + \"=\"*80)\nprint(\"QUALITATIVE EVALUATION: Baseline vs Fine-tuned\")\nprint(\"=\"*80)\n\nfor i, question in enumerate(test_questions, 1):\n    print(f\"\\n{'='*80}\")\n    print(f\"Question {i}: {question}\")\n    print(\"=\"*80)\n    \n    # Baseline response\n    print(\"\\n BASELINE (Pre-trained):\")\n    baseline_response = generate_response(baseline_model, question)\n    print(baseline_response)\n    \n    # Fine-tuned response\n    print(\"\\n FINE-TUNED (Medical):\")\n    finetuned_response = generate_response(fine_tuned_model, question)\n    print(finetuned_response)\n    \n    print(\"\\n\" + \"-\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T08:32:31.973505Z","iopub.execute_input":"2026-02-15T08:32:31.974247Z","iopub.status.idle":"2026-02-15T08:32:55.589063Z","shell.execute_reply.started":"2026-02-15T08:32:31.974217Z","shell.execute_reply":"2026-02-15T08:32:55.588368Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nQUALITATIVE EVALUATION: Baseline vs Fine-tuned\n================================================================================\n\n================================================================================\nQuestion 1: What are the symptoms of diabetes?\n================================================================================\n\n BASELINE (Pre-trained):\nThe symptoms of diabetes can vary depending on the type of diabetes and the individual's overall health. However, here are some common symptoms:\n1. Blurred vision: Diabetes can cause blurred vision, especially in the early stages.\n2. Thirst: People with diabetes may experience increased thirst and urination, which can lead to dehydration.\n3. Dry mouth: Diabetes can cause\n\n FINE-TUNED (Medical):\nDiabetes mellitus is characterized by the presence of hyperglycemia, which is a symptom.\n\n--------------------------------------------------------------------------------\n\n================================================================================\nQuestion 2: How is hypertension treated?\n================================================================================\n\n BASELINE (Pre-trained):\nHypertension is treated with medication, lifestyle changes, and sometimes surgery. Medication can help lower blood pressure by reducing the amount of blood that is pumped out of the heart. This can be done through medications such as blood pressure pills, anti-hypertensive medications, or medications that lower blood pressure through the kidneys. Lifestyle changes such as quitting smoking, losing weight, and maintaining a healthy diet can\n\n FINE-TUNED (Medical):\nHypertension is treated with medication.\n\n--------------------------------------------------------------------------------\n\n================================================================================\nQuestion 3: What causes pneumonia?\n================================================================================\n\n BASELINE (Pre-trained):\nPneumonia is caused by infection with bacteria, viruses, or fungi in the lungs. The bacteria that cause pneumonia are usually spread through the air, while viruses and fungi can be spread through contact with infected surfaces or droplets. The infection can also occur in people who have weakened immune systems or have underlying medical conditions that weaken their immune system.\n\n FINE-TUNED (Medical):\nPneumonia is caused by bacteria.\n\n--------------------------------------------------------------------------------\n\n================================================================================\nQuestion 4: Explain the difference between Type 1 and Type 2 diabetes.\n================================================================================\n\n BASELINE (Pre-trained):\nType 1 diabetes is a chronic metabolic disorder characterized by the absence or dysfunction of insulin-producing cells in the pancreas. This results in a lack of insulin, which leads to high blood sugar levels (hyperglycemia) and the development of complications such as blindness, kidney failure, and heart disease.\nOn the other hand, Type 2 diabetes is a metabolic disorder character\n\n FINE-TUNED (Medical):\nType 1 diabetes is characterized by autoimmune destruction of insulin-producing cells in the pancreas, while Type 2 diabetes is caused by insulin resistance.\n\n--------------------------------------------------------------------------------\n\n================================================================================\nQuestion 5: What are the side effects of aspirin?\n================================================================================\n\n BASELINE (Pre-trained):\nAspirin can cause the following side effects:\n\n1. Gastric bleeding: Aspirin can cause bleeding in the stomach or intestines, which can lead to severe pain, vomiting, and bleeding.\n\n2. Stomach ulcers: Aspirin can cause stomach ulcers, which are inflammation of the lining of the stomach or intestines.\n\n3. Heartburn: Asp\n\n FINE-TUNED (Medical):\nAspirin can cause gastric ulcers, which can lead to bleeding and perforation of the stomach lining.\n\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":117}]}